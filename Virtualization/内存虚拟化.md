# 内存虚拟化

## 初始化mmu

在创建vcpu的时候，会create每一个vcpu的mmu：

```C
int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
{
    kvm_mmu_create(vcpu);
    	-> __kvm_mmu_create(vpcu, &vcpu->arch.root_mmu);
    kvm_init_mmu(vcpu);
}
```

```C
static int __kvm_mmu_create(struct kvm_vcpu *vcpu, struct kvm_mmu *mmu) 
{
    mmu->root_hpa = INVALID_PAGE;
    mmu->root_pgd = 0;
    mmu->translate_gpa = translate_gpa;
}
```

`kvm_mmu_create`将mmu初始化成为了一个invalid的状态，这里的**root_hpa**指向的就是EPT页表中第一级页表的物理地址，这里先设置为INVALID，在vcpu_run之前调用的**kvm_mmu_reload**中创建根页表的地址。

紧接着又调用了`kvm_init_mmu`函数，这个函数根据当前启用的mmu去初始化相应的mmu：

```C
// arch/x86/kvm/mmu/mmu.c
void kvm_init_mmu(struct kvm_vcpu *vcpu, bool reset_roots)
{
	if (mmu_is_nested(vcpu))
		init_kvm_nested_mmu(vcpu);
	else if (tdp_enabled)
		init_kvm_tdp_mmu(vcpu);
	else
		init_kvm_softmmu(vcpu);
}
```

默认情况下是开启了`tdp_enabled`的，也就是开启了EPT的转换，所以调用`init_kvm_tdp_mmu`进行初始化：

```C
static void init_kvm_tdp_mmu(struct kvm_vcpu *vcpu)
{
	struct kvm_mmu *context = &vcpu->arch.root_mmu;

	context->page_fault = kvm_tdp_page_fault;
	context->sync_page = nonpaging_sync_page;
	context->shadow_root_level = kvm_mmu_get_tdp_level(vcpu);
	context->direct_map = true;
	context->get_guest_pgd = get_cr3;
	context->inject_page_fault = kvm_inject_page_fault;
    
    if (!is_cr0_pg(context))
		context->gva_to_gpa = nonpaging_gva_to_gpa;
	else if (is_cr4_pae(context))
		context->gva_to_gpa = paging64_gva_to_gpa;
	else
		context->gva_to_gpa = paging32_gva_to_gpa;
}
```

这里面初始化的几个最重要的参数是：

* page_fault: 指向了`kvm_tdp_page_fault`，这是在发生`page fault`之后所调用的函数；
* direct_map: `true`表示直接映射，也就是使用EPT，否则就使用影子页表。
* gva_to_gpa: 根据vcpu不同的状态设置成不同的转换函数。

## 虚拟机物理地址的设置

虚拟机的物理内存是通过VM类别的`ioctl(KVM_SET_USER_MEMORY_REGION)`设置的，其传递一个`struct user_space_memory_region`的结构：

```C
// 表示虚拟机的一段物理内存
struct kvm_userspace_memory_region {
	__u32 slot;		// 表示id号
	__u32 flags;
	__u64 guest_phys_addr;	// 虚拟的物理地址
	__u64 memory_size; /* bytes */
	__u64 userspace_addr; /* start of the userspace allocated memory */
};

/* for kvm_memory_region::flags */
#define KVM_MEM_LOG_DIRTY_PAGES	(1UL << 0)
#define KVM_MEM_READONLY	(1UL << 1)
```

`KVM_SET_USER_MEMORY_REGION`对应的处理函数如下：

```C
// virt/kvm/kvm_main.c
static int kvm_vm_ioctl_set_memory_region(struct kvm *kvm,
					  struct kvm_userspace_memory_region *mem)
{
	if ((u16)mem->slot >= KVM_USER_MEM_SLOTS)
		return -EINVAL;

	return kvm_set_memory_region(kvm, mem);
}
```

`kvm_set_memory_region`中继续调用`__kvm_set_memory_region`，该函数负责建立映射关系。

先将QEMU提供的内存slot转换为相应的索引，slot的高16位是`address space id (as_id)`，低16位是内存条的id号。这里`as_id`只可能是0或1，如果是普通虚拟机用的RAM，则为0，如果是SMM的模拟，则为1。`as_id`用来查找对应`address space`中的内存条: `kvm->memslots[as_id]`。

```C
// virt/kvm/kvm_main.c
int __kvm_set_memory_region(struct kvm *kvm,
			    const struct kvm_userspace_memory_region *mem)
{
    struct kvm_memory_slot old, new;
	struct kvm_memory_slot *tmp;
    
    as_id = mem->slot >> 16;
	id = (u16)mem->slot;
    
    // 一些完整性检查，例如内存大小和物理地址都要对齐，并且所有QEMU的虚拟地址需要可读。
    ...
 
	// 通过id得到对应内存条的引用
    tmp = id_to_memslot(__kvm_memslots(kvm, as_id), id);
	if (tmp) {
        // 如果有引用，那么拷贝一份出来，作为原来内存条的信息
		old = *tmp;
		tmp = NULL;
	} else {
        // 如果没有引用，证明是新增的内存条
		memset(&old, 0, sizeof(old));
		old.id = id;
	}
    
    // 建立一个新的内存条信息
    new.as_id = as_id;
	new.id = id;
	new.base_gfn = mem->guest_phys_addr >> PAGE_SHIFT;
	new.npages = mem->memory_size >> PAGE_SHIFT;
	new.flags = mem->flags;
	new.userspace_addr = mem->userspace_addr;
    
    if (!old.npages) {
        /* 原来没有内存条，是创建一个新的内存条的情况 */
		change = KVM_MR_CREATE;
		new.dirty_bitmap = NULL;
		memset(&new.arch, 0, sizeof(new.arch));
    } else {
        /* Modify an existing slot. */
    }
    
    // 检查是否有重叠
    if ((change == KVM_MR_CREATE) || (change == KVM_MR_MOVE)) {
		/* Check for overlaps */
		kvm_for_each_memslot(tmp, __kvm_memslots(kvm, as_id)) {
			if (tmp->id == id)
				continue;
			if (!((new.base_gfn + new.npages <= tmp->base_gfn) ||
			      (new.base_gfn >= tmp->base_gfn + tmp->npages)))
				return -EEXIST;
		}
	}
    
    r = kvm_set_memslot(kvm, mem, &old, &new, as_id, change);
}
```

然后调用`kvm_set_memslot`去设置新的内存条：

```C
static int kvm_set_memslot(struct kvm *kvm,
			   const struct kvm_userspace_memory_region *mem,
			   struct kvm_memory_slot *old,
			   struct kvm_memory_slot *new, int as_id,
			   enum kvm_mr_change change)
{
	struct kvm_memory_slot *slot;
	struct kvm_memslots *slots;
    
    // 分配一个新的memslots（如果是添加内存条，那么大小+1），将原来的那些memslots拷贝过来
    slots = kvm_dup_memslots(__kvm_memslots(kvm, as_id), change);
    
    // 分配arch中的rmap和lpage_info
    r = kvm_arch_prepare_memory_region(kvm, new, mem, change);
    
    // 将新的内存条new插入memslots中，内存条是按照gfn从大到小排序好的
    update_memslots(slots, new, change);
    
    // 插入新的内存条之后，使用RCU机制替换`kvm->memslots[as_id]`的指针，替换为刚刚分配的slots
    // 然后返回原来的__kvm_memslots(kvm, as_id)
	slots = install_new_memslots(kvm, as_id, slots);
    
    // 提交内存
    kvm_arch_commit_memory_region(kvm, mem, old, new, change);
    
    // 最后释放掉原来的memslots
    kvfree(slots);
}
```

`kvm_set_memslot`最后调用`kvm_arch_commit_memory_region`提交内存，这里主要是计算了一下当前虚拟机所需要的MMU页数，如果当前虚拟机已经使用的MMU页数超过了这个计算的页数，那么释放一部分MMU的页数。

### KVM中memslot的定义

这里先看以下KVM中内存条的几个定义：

* 首先是**kvm**结构体中定义的：

```C
struct kvm {
    // KVM_ADDRESS_SPACE_NUM = 2
	struct kvm_memslots __rcu *memslots[KVM_ADDRESS_SPACE_NUM];
};
```

* **kvm_memslots**:

```C
/*
 * Note:
 * memslots are not sorted by id anymore, please use id_to_memslot()
 * to get the memslot by its id.
 */
struct kvm_memslots {
	u64 generation;
	/* The mapping table from slot id to the index in memslots[]. */
	short id_to_index[KVM_MEM_SLOTS_NUM];
	atomic_t lru_slot;
	int used_slots;
	struct kvm_memory_slot memslots[];
};
```

* **kvm_memory_slot**:

```C
struct kvm_memory_slot {
	gfn_t base_gfn;		// guest 物理地址的起始pfn
	unsigned long npages;	// 这个内存条有多少个页
	unsigned long *dirty_bitmap;
	struct kvm_arch_memory_slot arch;
	unsigned long userspace_addr;
	u32 flags;
	short id;
	u16 as_id;
};
```

* **kvm_arch_memory_slot**

```C
struct kvm_arch_memory_slot {
	struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
	struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
	unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
};
```

## EPT页表的构建

* **tdp**: two dimission page.

### EPT根页表的创建

在`vcpu_enter_guest`之中，也就是在vcpu_run之前，调用了**kvm_mmu_reload**函数:

```C
static inline int kvm_mmu_reload(struct kvm_vcpu *vcpu)
{
	if (likely(vcpu->arch.mmu->root_hpa != INVALID_PAGE))
		return 0;

	return kvm_mmu_load(vcpu);
}
```

其中首先判断了`mmu->root_hpa`是不是等于`INVALID_PAGE`，如果不等于，证明已经创建了根页表，直接返回；否则调用`kvm_mmu_load`函数去创建根页表。

```C
int kvm_mmu_load(struct kvm_vcpu *vcpu)
{
    r = mmu_topup_memory_caches(vcpu, !vcpu->arch.mmu->direct_map);
    
    if (vcpu->arch.mmu->direct_map)
		r = mmu_alloc_direct_roots(vcpu);
    
    kvm_mmu_load_pgd(vcpu);
	static_call(kvm_x86_tlb_flush_current)(vcpu);
}
```

首先是去填充了`mem_cache`，这是分配内存时的优化，需要页的时候优先从`mem_cache`中拿页。

因为我们用了EPT，所以是`direct_map`，所以调用`mmu_alloc_direct_roots`去分配根页表相应的页:

```C
static int mmu_alloc_direct_roots(struct kvm_vcpu *vcpu)
{
    if (is_tdp_mmu_enabled(vcpu->kvm)) {
		root = kvm_tdp_mmu_get_vcpu_root_hpa(vcpu);
		mmu->root_hpa = root;
    }
}   
```

```C
hpa_t kvm_tdp_mmu_get_vcpu_root_hpa(struct kvm_vcpu *vcpu)
{
    root = alloc_tdp_mmu_page(vcpu, 0, vcpu->arch.mmu->shadow_root_level);
    	-> sp->spt = kvm_mmu_memory_cache_alloc(&vcpu->arch.mmu_shadow_page_cache);
    
    return __pa(root->spt);
}
```

可以看到，分配的根页表的页保存在了`root->spt`中，并且返回了其物理地址。

再然后，`kvm_mmu_load`继续调用了`kvm_mmu_load_pgd`：

```C
static inline void kvm_mmu_load_pgd(struct kvm_vcpu *vcpu)
{
	u64 root_hpa = vcpu->arch.mmu->root_hpa;

	if (!VALID_PAGE(root_hpa))
		return;

	static_call(kvm_x86_load_mmu_pgd)(vcpu, root_hpa,
					  vcpu->arch.mmu->shadow_root_level);
}
```

这里调用到了vmx下的`vmx_load_mmu_pgd`：

```C
static void vmx_load_mmu_pgd(struct kvm_vcpu *vcpu, hpa_t root_hpa,
			     int root_level)
{
    if (enable_ept) {
		eptp = construct_eptp(vcpu, root_hpa, root_level);
		vmcs_write64(EPT_POINTER, eptp);
    }
}
```

使用`root_hpa`构建了eptp，并且写入到了VMCS中的`EPT_POINTER`。

### EPT缺页处理

EPT缺页的时候会发生`ept violation`，这会调用到`handle_ept_violation`去处理：

```C
static int handle_ept_violation(struct kvm_vcpu *vcpu)
{
    exit_qualification = vmx_get_exit_qual(vcpu);
    gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
    
    /* Is it a read fault? */
	error_code = (exit_qualification & EPT_VIOLATION_ACC_READ)
		     ? PFERR_USER_MASK : 0;
	/* Is it a write fault? */
	error_code |= (exit_qualification & EPT_VIOLATION_ACC_WRITE)
		      ? PFERR_WRITE_MASK : 0;
	/* Is it a fetch fault? */
	error_code |= (exit_qualification & EPT_VIOLATION_ACC_INSTR)
		      ? PFERR_FETCH_MASK : 0;
	/* ept page table entry is present? */
	error_code |= (exit_qualification &
		       (EPT_VIOLATION_READABLE | EPT_VIOLATION_WRITABLE |
			EPT_VIOLATION_EXECUTABLE))
		      ? PFERR_PRESENT_MASK : 0;
    
    vcpu->arch.exit_qualification = exit_qualification;
    
    return kvm_mmu_page_fault(vcpu, gpa, error_code, NULL, 0);
}
```

```C
// arch/x86/kvm/mmu/mmu.c
int kvm_mmu_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa, u64 error_code,
		       void *insn, int insn_len)
{
    r = kvm_mmu_do_page_fault(vcpu, cr2_or_gpa,
					  lower_32_bits(error_code), false);
}
```

```C
static inline int kvm_mmu_do_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,
					u32 err, bool prefault)
{
    return kvm_tdp_page_fault(vcpu, cr2_or_gpa, err, prefault);
}
```

最终调用了`kvm_tdp_page_fault`函数去处理，其中又调用了`direct_page_fault`去处理：

```C
// arch/x86/kvm/mmu/mmu.c
static int direct_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
			     bool prefault, int max_level, bool is_tdp)
{
    if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, &hva,
			 write, &map_writable))
		return RET_PF_RETRY;
}
```

紧接着又调用了`try_async_pf`来对页错误进行异步处理：

```C
// arch/x86/kvm/mmu/mmu.c
static bool try_async_pf(struct kvm_vcpu *vcpu, bool prefault, gfn_t gfn,
			 gpa_t cr2_or_gpa, kvm_pfn_t *pfn, hva_t *hva,
			 bool write, bool *writable)
{
	struct kvm_memory_slot *slot = kvm_vcpu_gfn_to_memslot(vcpu, gfn);
	bool async;

	/*
	 * Retry the page fault if the gfn hit a memslot that is being deleted
	 * or moved.  This ensures any existing SPTEs for the old memslot will
	 * be zapped before KVM inserts a new MMIO SPTE for the gfn.
	 */
	if (slot && (slot->flags & KVM_MEMSLOT_INVALID))
		return true;

	async = false;
	*pfn = __gfn_to_pfn_memslot(slot, gfn, false, &async,
				    write, writable, hva);
	if (!async)
		return false; /* *pfn has correct page already */

	if (!prefault && kvm_can_do_async_pf(vcpu)) {
		trace_kvm_try_async_get_page(cr2_or_gpa, gfn);
		if (kvm_find_async_pf_gfn(vcpu, gfn)) {
			trace_kvm_async_pf_doublefault(cr2_or_gpa, gfn);
			kvm_make_request(KVM_REQ_APF_HALT, vcpu);
			return true;
		} else if (kvm_arch_setup_async_pf(vcpu, cr2_or_gpa, gfn))
			return true;
	}

	*pfn = __gfn_to_pfn_memslot(slot, gfn, false, NULL,
				    write, writable, hva);
	return false;
}
```

首先是根据错误的gfn找到对应的memslot，这里的实现是通过二分查找实现的，在之前说过，插入内存条的时候就是按照gfn从大到小排序的。然后判断内存条是否是`KVM_MEMSLOT_INVALID`的，如果是的，那么说明当前这个内存条正在删除或者是移动。这里先要说明一下在删除或移动内存条的时候，首先就先把内存条的flags上设置上`KVM_MEMSLOT_INVALID`，然后才去进行清除该内存条在EPT中所对应的那些页(SPTEs)，然后KVM才能安全的插入一个新的MMIO的SPTE对这个gfn。

然后调用`__gfn_to_pfn_memslot`去从Virtual address中获取一个物理地址回来：

```C
kvm_pfn_t __gfn_to_pfn_memslot(struct kvm_memory_slot *slot, gfn_t gfn,
			       bool atomic, bool *async, bool write_fault,
			       bool *writable, hva_t *hva)
{
	unsigned long addr = __gfn_to_hva_many(slot, gfn, NULL, write_fault);

	if (hva)
		*hva = addr;

    // some check
	...

	return hva_to_pfn(addr, atomic, async, write_fault,
			  writable);
}
```

然后调用`__gfn_to_hva_many`将gfn转换为host的虚拟地址：

```C
static unsigned long __gfn_to_hva_many(struct kvm_memory_slot *slot, gfn_t gfn,
				       gfn_t *nr_pages, bool write)
{
	return __gfn_to_hva_memslot(slot, gfn);
}

static inline unsigned long
__gfn_to_hva_memslot(const struct kvm_memory_slot *slot, gfn_t gfn)
{
	/*
	 * The index was checked originally in search_memslots.  To avoid
	 * that a malicious guest builds a Spectre gadget out of e.g. page
	 * table walks, do not let the processor speculate loads outside
	 * the guest's registered memslots.
	 */
	unsigned long offset = gfn - slot->base_gfn;
	offset = array_index_nospec(offset, slot->npages);
	return slot->userspace_addr + offset * PAGE_SIZE;
}
```

可以看到，这里计算出了gfn相对于当前内存条base_gfn的偏移，然后直接加上了`userspace_addr`，得到了对应的host的虚拟地址，然后返回hva到`__gfn_to_pfn_memslot`函数中，再接着往下看，在做了一些权限的检查之后，继续调用`hva_to_pfn`函数。

```C
static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,
			bool write_fault, bool *writable)
{
    // 通过调用get_user_page相关的函数，将host va对应的pfn返回回来
    if (hva_to_pfn_fast(addr, write_fault, writable, &pfn))
		return pfn;
    
    // 如果没获得pfn的话，将async设置为true
}
```

这里就两种情况，一种是如果pin住了user的page并且返回了对应的pfn，那么ok，返回到`direct_page_fault`中，继续执行。如果没有pin到user的page，说明当前的物理页可能被swap出去了，那么这个时候需要设置`async`为true，证明需要异步的处理，那么让虚拟机去调度其他的进程。

假设成功get到了物理页，那么返回到`direct_page_fault`继续执行：

```C
static int direct_page_fault(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
			     bool prefault, int max_level, bool is_tdp)
{
    if (try_async_pf(vcpu, prefault, gfn, gpa, &pfn, &hva,
			 write, &map_writable))
		return RET_PF_RETRY;
    
    if (is_tdp_mmu_fault)
		r = kvm_tdp_mmu_map(vcpu, gpa, error_code, map_writable, max_level,
				    pfn, prefault);
}
```

如果是tdp的mmu错误，那么调用**kvm_tdp_mmu_map**去建立相应的映射。

```C
int kvm_tdp_mmu_map(struct kvm_vcpu *vcpu, gpa_t gpa, u32 error_code,
		    int map_writable, int max_level, kvm_pfn_t pfn,
		    bool prefault)
{
    // 遍历ept页表，找到要建立映射的entry
    tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {
		if (nx_huge_page_workaround_enabled)
			disallowed_hugepage_adjust(iter.old_spte, gfn,
						   iter.level, &pfn, &level);

		if (iter.level == level)
			break;

		/*
		 * If there is an SPTE mapping a large page at a higher level
		 * than the target, that SPTE must be cleared and replaced
		 * with a non-leaf SPTE.
		 */
		if (is_shadow_present_pte(iter.old_spte) &&
		    is_large_pte(iter.old_spte)) {
			if (!tdp_mmu_zap_spte_atomic(vcpu->kvm, &iter))
				break;

			/*
			 * The iter must explicitly re-read the spte here
			 * because the new value informs the !present
			 * path below.
			 */
			iter.old_spte = READ_ONCE(*rcu_dereference(iter.sptep));
		}

        // 遍历页表的过程中给页表建立页
		if (!is_shadow_present_pte(iter.old_spte)) {
			/*
			 * If SPTE has been frozen by another thread, just
			 * give up and retry, avoiding unnecessary page table
			 * allocation and free.
			 */
			if (is_removed_spte(iter.old_spte))
				break;

            // 分配一个新的页表页
			sp = alloc_tdp_mmu_page(vcpu, iter.gfn, iter.level - 1);
			child_pt = sp->spt;

			new_spte = make_nonleaf_spte(child_pt,
						     !shadow_accessed_mask);

			if (tdp_mmu_set_spte_atomic(vcpu->kvm, &iter,
						    new_spte)) {
				tdp_mmu_link_page(vcpu->kvm, sp, true,
						  huge_page_disallowed &&
						  req_level >= iter.level);

				trace_kvm_mmu_get_page(sp, true);
			} else {
				tdp_mmu_free_sp(sp);
				break;
			}
		}
	}
    
    // 最后真正的将映射写入对应的entry
	ret = tdp_mmu_map_handle_target_level(vcpu, write, map_writable, &iter,
					      pfn, prefault);
}
```



## 一些函数的记录

```C
#define for_each_tdp_pte_min_level(iter, root, min_level, start, end) \
	for (tdp_iter_start(&iter, root, min_level, start); \
		 iter.valid && iter.gfn < end;	\
		 tdp_iter_next(&iter))

#define for_each_tdp_pte(iter, root, start, end) \
	for_each_tdp_pte_min_level(iter, root, PG_LEVEL_4K, start, end)

#define tdp_mmu_for_each_pte(_iter, _mmu, _start, _end)	\
	for_each_tdp_pte(_iter, to_shadow_page(_mmu->root.hpa), _start, _end)

struct tdp_iter iter;
tdp_mmu_for_each_pte(iter, mmu, gfn, gfn + 1) {

}

for (tdp_iter_start(&iter, to_shadow_page(_mmu->root.hpa), PG_LEVEL_4K, gfn);
     iter.valid && iter.gfn < (gfn + 1);
     tdp_iter_next(&iter))
```

