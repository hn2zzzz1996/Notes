# 8. Slab Allocator

Why memcache or slab allocator comes out? Due to in many cases the cost of initializing and destroying the object exceeds the cost of allocating and freeing memory for it. So caching the frequently used object is a big improvement for process.

Each object in the cache has the same size, so the only wasted space is the unused portion at the end of the slab. (Internal fragmentation)

The slab allocator has three principle aims:

* Eliminate internal fragmentation that would be caused by the buddy system;
* The caching of commonly used objects so that the system does not waste time allocating, initialising and destroying objects.
* The better utilisation of hardware cache by aligning objects to the L1 or L2 caches.

### Slab coloring

The slab allocator incorporate a simple coloring scheme that resulting in excellent cache utilization.

The concept is simple: each time a new slab is created, the buffer addresses start at a slightly different offset (color) from the slab base (which is always page-aligned). For example:

For a cache of 200-byte objects with 8-byte alignment, the first slab's buffers would be at addresses 0, 200, 400, ... relative to the slab base. The next slab's buffers would be at offsets 8, 208, 408, ... and so on. 

**And why this works, why it can reduce the cache miss and improve the performance?**

Here is the same question and the answer in it.

https://stackoverflow.com/questions/46731933/linux-slab-allocator-and-cache-performance

We know that cache will be divided into different sets. And the calculation formula to decide which set a memory block (A memory block will include several bytes, ) will be in is:

> (Block address) Mod (Number of sets in cache)

So let's see an example, first take a memory block `0x1000008` (from slabX with color C) and memory block address `0x2000009` (from slabY with color Z). For most `N` (number of sets in cache), the calculation for `<address> MOD <N>` will yield a different value, hence a different set to cache the data.

If no offset in slab object, the object in different slab will always locate into a same set (Due to slabs are always page aligned, so the two object may have memory block `0x100008` and `0x200008`, and locate in the same set). So by keeping a different offset (colors) for the objects in different slabs, the objects will locate into different cache sets, so the performance has been improved.

## Cache

### Cache Desciptor

All information describing a cache is stored in a struct `kmem_cache_s` declared in mm/slab.c.

```C
190 struct kmem_cache_s {
193     struct list_head        slabs_full;
194     struct list_head        slabs_partial;
195     struct list_head        slabs_free;
196     unsigned int            objsize;
197     unsigned int            flags;	// Determine how parts of the allocator will behave when dealing with the cache.
198     unsigned int            num;	// The number of objects contained in each slab
199     spinlock_t              spinlock;	// protecting the structure
200 #ifdef CONFIG_SMP
201     unsigned int            batchcount;	// This is the number of objects that will be allocated in batch for the per-cpu caches.
202 #endif
203 
206     unsigned int            gfporder;	// The size of the slab in pages.
209     unsigned int            gfpflags;	// Used for calling buddy allocator
210 
211     size_t                  colour;
212     unsigned int            colour_off;
213     unsigned int            colour_next;	// The next colour line to use
    // When the slab descriptor is out off the slab page, the slabp_cache will point to one of the size cache that is used for allocated the slab descriptor. Can see the kmem_find_general_cachep() for detail.
214     kmem_cache_t            *slabp_cache;
215     unsigned int            growing;	// If the cache is growing or not. If it is, it's much less likely this cache will be selected to reap free slabs under memory pressure.
216     unsigned int            dflags;	// Dynamic flags which change during the cache lifetime.
217 
219     void (*ctor)(void *, kmem_cache_t *, unsigned long);
222     void (*dtor)(void *, kmem_cache_t *, unsigned long);
223 
224     unsigned long           failures;
225 
227     char                    name[CACHE_NAMELEN];	// The name that kmem_cache_create() passed in
228     struct list_head        next;	// The next cache on the cache chain.

229 #ifdef CONFIG_SMP
231     cpucache_t              *cpudata[NR_CPUS];	// The percpu data.
232 #endif

233 #if STATS
    // The current number of active objects in the cache is stored here;
234     unsigned long           num_active;
    // The number of objects that have been allocated.
235     unsigned long           num_allocations;
    // The highest value num_active has had to date.
236     unsigned long           high_mark;
    // The number of times kmem_cache_grow() has been called.
237     unsigned long           grown;
    // The number of times this cache has been reaped.
238     unsigned long           reaped;
239     unsigned long           errors;
240 #ifdef CONFIG_SMP
    // The total number of times an allocation from the per-cpu cache;
241     atomic_t                allochit;
    // The per-cpu cache is empty, so allocmiss++.
242     atomic_t                allocmiss;
    // When free a object, if it put into the per-cpu cache, freehit++.
243     atomic_t                freehit;
    // When free a object, the per-cpu cache has been full, so freemiss++.
244     atomic_t                freemiss;
245 #endif
246 #endif
247 };
```

### Cache Static Flags

There are three principle sets. All defined in <linux/slab.h>.

The **first** set ins internal flags:

| Flag          | Description                                                  |
| ------------- | ------------------------------------------------------------ |
| CFGS_OFF_SLAB | This determine where the slab descriptor is stored. If it's not off_slab, means the slab descriptor will be locate in the slab pages. If it's off_slab, means the slab descriptor should be allocated in another place. |

The second set are set by the cache creator and they determine how the allocator treats the slab and how objects are stored.

| Flag               | Description                              |
| ------------------ | ---------------------------------------- |
| SLAB_HWCACHE_ALIGN | Align the objects to the L1 CPU cache.   |
| SLAB_NO_REAP       | Never reap slabs in this cache.          |
| SLAB_CACHE_DMA     | Allocate slabs with memory from ZONE_DMA |

The last flags are only available if the compile option `CONFIG_SLAB_DEBUG` is set. They determine that additional checks will be made to slabs and objects.

| Flag               | Description                                                  |
| ------------------ | ------------------------------------------------------------ |
| SLAB_DEBUG_FREE    | Perform expensive checks on free                             |
| SLAB_DEBUG_INITIAL | On free, call the constructor as a verifier to ensure the object is still initialized correctly |
| SLAB_RED_ZONE      | This places a marker at either end of objects to trap overflows |
| SLAB_POISON        | Poison objects with a known pattern for trapping changes made to objects not allocated or initialized. |

### Cache Dynamic Flags

The `dflags` field has only one flag, `DFLGS_GROWN`. The flag is set during `kmem_cache_grow()` so that `kmem_cache_reap()` will be unlikely to choose the cache for reaping. When the function does find a cache with this flag set, it skips the cache and removes the flag.

## Slabs

This section will describe how a slab is structured and managed.

```C
typedef struct slab_s {
    // This will be linked one of slab_full, slab_partial or slab_free from cache manager.
    struct list_head        list;
    // This is the colour offset from the base address of the first object within the slab. The address of the first object is s_mem + colouroff.
    unsigned long           colouroff;
    // This is the start address of the first object within the slab.
    void                    *s_mem;
    // The number of active objects in the slab.
    unsigned int            inuse;
	// kmem_bufctl_t is a typedef os unsigned int.
    // This the index which determine the head the free list.
    kmem_bufctl_t           free;
} slab_t;
```

Here is a question, Given a slab manager or an object, how to determine what slab or cache they belong to?

This is addressed by using the `list` field in the `struct page`. The `list.prev` is used to store the cache it belongs to, and the `list.next` is used to store the slab it belongs to.

### Storing the Slab Descriptor

If the objects are larger than a threshold (512 bytes on x86, which is 1/8 of PAGE_SIZE), `CFGS_OFF_SLAB` is set in the cache flags and the `slab descriptor` will be out of the slab pages. This limits the number of objects that can be stored on the slab because there is `limited space for the bufctls`. But that's unimportant as the objects are large and so there should not be many stored in a single slab.

![Slab With Descriptor On-Slab](.\picture\Slab With Descriptor On-Slab.png)

Alternatively, the slab manager is reserved at the beginning of the slab. When stored on-slab, enough space is kept at the beginning of the slab to store both the `slab_t` and the `kmem_bufctl_t` which is an array of unsigned integers. The array is responsible for tracking the index of the next free object that is available for use. The actual objects are stored after the `kmem_bufctl_t` array.

![Slab With Descriptor Off-Slab](.\picture\Slab With Descriptor Off-Slab.png)

### Tracking Free Objects

The `kmem_bufctl_t` array is used to track the free objects linked list. The bufctl[i] is means: If the current free object is i, and we allocate it, so the next free object will be bufctl[i].

That's tracked by `slab_t->free`, which is initialized to 0. And means the 0th object is the first free object to be used. And the bufctl will be initialized like below:

![kmem_bufctl_t](.\picture\kmem_bufctl_t.png)

So it's a free list but recorded as array.

## Sizes cache

Linux keeps two sets of caches for small memory allocations for which the physical page allocator is unsuitable. One set is for use with DMA and the other is suitable for normal use. They named as `size-N` cache and `size-N(DMA)` cache. A `struct cache_sizes` is been used to describe them:

```C
331 typedef struct cache_sizes {
    // The size of the memory block;
332     size_t           cs_size;
    // The cache of blocks for normal memory use;
333     kmem_cache_t    *cs_cachep;
    // The cache of blocks for use with DMA.
334     kmem_cache_t    *cs_dmacachep;
335 } cache_sizes_t;
```

And there are a limited number of these caches that exist, a static array called `cache_sizes` is initialized at beginning time:

```C
337 static cache_sizes_t cache_sizes[] = {
338 #if PAGE_SIZE == 4096
339     {    32,        NULL, NULL},
340 #endif
341     {    64,        NULL, NULL},
342     {   128,        NULL, NULL},
343     {   256,        NULL, NULL},
344     {   512,        NULL, NULL},
345     {  1024,        NULL, NULL},
346     {  2048,        NULL, NULL},
347     {  4096,        NULL, NULL},
348     {  8192,        NULL, NULL},
349     { 16384,        NULL, NULL},
350     { 32768,        NULL, NULL},
351     { 65536,        NULL, NULL},
352     {131072,        NULL, NULL},
353     {     0,        NULL, NULL}
```

This is range from 2^5 to 2^17.

## Per-CPU Object Cache

An aim of high performance computing is to use data on the same CPU as long as possible. Linux achieves this by trying to keep objects in the same CPU cache with a Per-CPU object cache, simply called a cpucache for each CPU in the system.

This has provide two advanteges:

1. The hardware cache will be used for as long as possible on the same CPU.
2. Reduce the use of lock. When object allocated from per-cpu cache, there is no need to get the lock.

### Describing the Per-CPU Object Cache

Each cache descriptor has a pointer to an array of cpucaches, described in the cache descriptor as

```C
231    cpucache_t              *cpudata[NR_CPUS];
```

This structure is very simple

```C
173 typedef struct cpucache_s {
174     unsigned int avail; // The number of free objects available on this cpucache.
175     unsigned int limit; // The total number of free objects that can exist.
176 } cpucache_t;
```

A helper macro to get the cpucache.

```C
180 #define cc_data(cachep) \
181         ((cachep)->cpudata[smp_processor_id()])
```

But where is the object? The pointers to objects on the cpucache are placed immediately after the `cpucache_t` struct. 

### Adding/Removing Objects from the Per-CPU Cache

To add an object (`obj`) to the CPU cache (`cc`), the following block of code is used

```
        cc_entry(cc)[cc->avail++] = obj;
```

To remove an object

```
        obj = cc_entry(cc)[--cc->avail];
```

The macro `cc_entry` is helped to get the pointer of object in the cpucahce. It's defined as:

```C
178 #define cc_entry(cpucache) \
179         ((void **)(((cpucache_t*)(cpucache))+1))
```

## Slab Allocator Initialization

When the slab allocator creates a new cache, it allocates the `kmem_cache_t` from the `cache_cache` or `kmem_cache` cache. This is an obvious chicken and egg problem so the `cache_cache` has to be statically initialised as:

```C
357 static kmem_cache_t cache_cache = {
358     slabs_full:     LIST_HEAD_INIT(cache_cache.slabs_full),
359     slabs_partial:  LIST_HEAD_INIT(cache_cache.slabs_partial),
360     slabs_free:     LIST_HEAD_INIT(cache_cache.slabs_free),
361     objsize:        sizeof(kmem_cache_t),
362     flags:          SLAB_NO_REAP,
363     spinlock:       SPIN_LOCK_UNLOCKED,
364     colour_off:     L1_CACHE_BYTES,
365     name:           "kmem_cache",
366 };
```

To initialize the rest of the struct, `kmem_cache_init()` is called from `start_kernel()`.

## Function

### 1 Cache Manipulation

#### 1.1 Cache Creation

##### 1.1.1 Function: kmem_cache_create (mm/slab.c)

```C
621 kmem_cache_t *
    // @offset: This is used to specify a specific alignment for objects in the cache but it usually left as 0.
622 kmem_cache_create (const char *name, size_t size, 
623     size_t offset, unsigned long flags, 
        void (*ctor)(void*, kmem_cache_t *, unsigned long),
624     void (*dtor)(void*, kmem_cache_t *, unsigned long))
625 {
626     const char *func_nm = KERN_ERR "kmem_create: ";
627     size_t left_over, align, slab_size;
628     kmem_cache_t *cachep = NULL;
629 
633     if ((!name) ||
634         ((strlen(name) >= CACHE_NAMELEN - 1)) ||
635         in_interrupt() ||
636         (size < BYTES_PER_WORD) ||
637         (size > (1<<MAX_OBJ_ORDER)*PAGE_SIZE) ||
638         (dtor && !ctor) ||
639         (offset < 0 || offset > size))
640             BUG();
641 
642 #if DEBUG
643     if ((flags & SLAB_DEBUG_INITIAL) && !ctor) {
645         printk("%sNo con, but init state check 
                requested - %s\n", func_nm, name);
646         flags &= ~SLAB_DEBUG_INITIAL;
647     }
648 
649     if ((flags & SLAB_POISON) && ctor) {
651         printk("%sPoisoning requested, but con given - %s\n",
                                                  func_nm, name);
652         flags &= ~SLAB_POISON;
653     }
654 #if FORCED_DEBUG
655     if ((size < (PAGE_SIZE>>3)) && 
        !(flags & SLAB_MUST_HWCACHE_ALIGN))
660         flags |= SLAB_RED_ZONE;
661     if (!ctor)
662         flags |= SLAB_POISON;
663 #endif
664 #endif
670     BUG_ON(flags & ~CREATE_MASK);

673     cachep = 
           (kmem_cache_t *) kmem_cache_alloc(&cache_cache,
                         SLAB_KERNEL);
674     if (!cachep)
675         goto opps;
	// Zero fill the object.
676     memset(cachep, 0, sizeof(kmem_cache_t));

	// Align to word size
682     if (size & (BYTES_PER_WORD-1)) {
683         size += (BYTES_PER_WORD-1);
684         size &= ~(BYTES_PER_WORD-1);
685         printk("%sForcing size word alignment 
               - %s\n", func_nm, name);
686     }
687
688 #if DEBUG
689     if (flags & SLAB_RED_ZONE) {
694         flags &= ~SLAB_HWCACHE_ALIGN;
695         size += 2*BYTES_PER_WORD;
696     }
697 #endif
698     align = BYTES_PER_WORD;
	// If requested, align the objects to the L1 CPU cache
699     if (flags & SLAB_HWCACHE_ALIGN)
700         align = L1_CACHE_BYTES;
701
	// If the objects are large, now the threshold is 512Kb. It will store the slab descriptors off-slab based on the CFLGS_OFF_SLAB. This allow better packing of objects into the slab.
703     if (size >= (PAGE_SIZE>>3))
708         flags |= CFLGS_OFF_SLAB;
709 
	// For machines with large L1 cache lines, two or more small objects may fit into each line.
710     if (flags & SLAB_HWCACHE_ALIGN) {
714         while (size < align/2)
715             align /= 2;
716         size = (size+align-1)&(~(align-1));
717     }

	// Calculate how many objects will fit on a slab and adjust the slab size as necessary.
724     do {
725         unsigned int break_flag = 0;
726 cal_wastage:
    // The kmem_cache_esimate() calculates the number of objects that can fit on a slab at the current gfp order and what amount of leftover bytes will be.
    // In the first round, the cachep->gfporder is 0
727         kmem_cache_estimate(cachep->gfporder, 
                    size, flags,
728                     &left_over, 
                    &cachep->num);
729         if (break_flag)
730             break;
731         if (cachep->gfporder >= MAX_GFP_ORDER)
732             break;
733         if (!cachep->num)
734             goto next;
735         if (flags & CFLGS_OFF_SLAB && 
            cachep->num > offslab_limit) {
737             cachep->gfporder--;
738             break_flag++;
739             goto cal_wastage;
740         }
741 
746         if (cachep->gfporder >= slab_break_gfp_order)
747             break;
748 
749         if ((left_over*8) <= (PAGE_SIZE<<cachep->gfporder))
750             break;  
751 next:
752         cachep->gfporder++;
753     } while (1);
754
755     if (!cachep->num) {
756         printk("kmem_cache_create: couldn't 
                create cache %s.\n", name);
757         kmem_cache_free(&cache_cache, cachep);
758         cachep = NULL;
759         goto opps;
760     }
	// slab_size is the total size of the slab descriptor.
761     slab_size = L1_CACHE_ALIGN(
              cachep->num*sizeof(kmem_bufctl_t) + 
              sizeof(slab_t));
762 
767     if (flags & CFLGS_OFF_SLAB && left_over >= slab_size) {
768         flags &= ~CFLGS_OFF_SLAB;
769         left_over -= slab_size;
770     }

	// Calculate colour offsets.
773     offset += (align-1);
774     offset &= ~(align-1);
775     if (!offset)
776         offset = L1_CACHE_BYTES;
777     cachep->colour_off = offset;
778     cachep->colour = left_over/offset;
	// Initialise remaining fields in cache descriptor
781     if (!cachep->gfporder && !(flags & CFLGS_OFF_SLAB))
782         flags |= CFLGS_OPTIMIZE;
783 
784     cachep->flags = flags;
785     cachep->gfpflags = 0;
786     if (flags & SLAB_CACHE_DMA)
787         cachep->gfpflags |= GFP_DMA;
788     spin_lock_init(&cachep->spinlock);
789     cachep->objsize = size;
790     INIT_LIST_HEAD(&cachep->slabs_full);
791     INIT_LIST_HEAD(&cachep->slabs_partial);
792     INIT_LIST_HEAD(&cachep->slabs_free);
793 
794     if (flags & CFLGS_OFF_SLAB)
795         cachep->slabp_cache =
               kmem_find_general_cachep(slab_size,0);
796     cachep->ctor = ctor;
797     cachep->dtor = dtor;
799     strcpy(cachep->name, name);
800 
801 #ifdef CONFIG_SMP
802     if (g_cpucache_up)
803         enable_cpucache(cachep);
804 #endif

806     down(&cache_chain_sem);
807     {
    // Add the new cache to the cache chain.
808         struct list_head *p;
809 
810         list_for_each(p, &cache_chain) {
811             kmem_cache_t *pc = list_entry(p, 
                    kmem_cache_t, next);
812 
814             if (!strcmp(pc->name, name))
815                 BUG();
816         }
817     }
818 
822     list_add(&cachep->next, &cache_chain);
823     up(&cache_chain_sem);
824 opps:
825     return cachep;
826 }
```

#### 1.2 Calculating the Number of Objects on a Slab

##### 1.2.1 Function: kmem_cache_estimate (mm/slab.c)

```C
388 static void kmem_cache_estimate (unsigned long gfporder, 
             size_t size,
389          int flags, size_t *left_over, unsigned int *num)
390 {
391     int i;
392     size_t wastage = PAGE_SIZE<<gfporder;
    // extra is the number of bytes needed to store kmem_bufctl_t
393     size_t extra = 0;	
    // base is where usable memory in the slab starts
394     size_t base = 0;
395 
    // If the slab descriptor is kept on cache, the base begins at the end of the slab_t struct and the number of bytes needed to store the bufctl is the size of kmem_bufctl_t.
396     if (!(flags & CFLGS_OFF_SLAB)) {
397         base = sizeof(slab_t);
398         extra = sizeof(kmem_bufctl_t);
399     }
    // i means the number of objects the slab can hold
400     i = 0;
    // This counts up the number of objects that the cache can store.
401     while (i*size + L1_CACHE_ALIGN(base+i*extra) <= wastage)
402         i++;
403     if (i > 0)
404         i--;
405 
406     if (i > SLAB_LIMIT)
407         i = SLAB_LIMIT;
408 
409     *num = i;
410     wastage -= i*size;
411     wastage -= L1_CACHE_ALIGN(base+i*extra);
412     *left_over = wastage;
413 }
```

#### 1.5 Cache Reaping

##### 1.5.1 Function: kmem_cache_reap (mm/slab.c)

This function is responsible to reclaim some pages from the cache when memory is under pressure. And only be called by `kswapd`. It will select a cache that will be required to shrink its memory usage.

But this function has a problem that cache reaping does not take into account what memory node or zone is under pressure. This means that with a NUMA machine, it is possible the kernel will spend a lot of time freeing memory from regions that are under no memory pressure.

```C
1738 int kmem_cache_reap (int gfp_mask)
1739 {
1740     slab_t *slabp;
1741     kmem_cache_t *searchp;
1742     kmem_cache_t *best_cachep;
1743     unsigned int best_pages;
1744     unsigned int best_len;
1745     unsigned int scan;
1746     int ret = 0;
1747 
    // The check is worthless, due to the only caller kswapd always can sleep.
1748     if (gfp_mask & __GFP_WAIT)
1749         down(&cache_chain_sem);
1750     else
1751         if (down_trylock(&cache_chain_sem))
1752             return 0;
1753 
    // REAP_SCANLEN(10) is the number of caches to examine.
1754     scan = REAP_SCANLEN;
1755     best_len = 0;
1756     best_pages = 0;
1757     best_cachep = NULL;
    // Set searchp to be the last cache that was examined at the last reap
1758     searchp = clock_searchp;

1759     do {
1760         unsigned int pages;
1761         struct list_head* p;
1762         unsigned int full_free;
1763 
1765         if (searchp->flags & SLAB_NO_REAP)
1766             goto next;
1767         spin_lock_irq(&searchp->spinlock);
1768         if (searchp->growing)
1769             goto next_unlock;
1770         if (searchp->dflags & DFLGS_GROWN) {
1771             searchp->dflags &= ~DFLGS_GROWN;
1772             goto next_unlock;
1773         }
1774 #ifdef CONFIG_SMP
1775         {
1776             cpucache_t *cc = cc_data(searchp);
1777             if (cc && cc->avail) {
1778                 __free_block(searchp, cc_entry(cc),
                          cc->avail);
1779                 cc->avail = 0;
1780             }
1781         }
1782 #endif
1783 
1784         full_free = 0;
1785         p = searchp->slabs_free.next;
1786         while (p != &searchp->slabs_free) {
1787             slabp = list_entry(p, slab_t, list);
1788 #if DEBUG
1789             if (slabp->inuse)
1790                 BUG();
1791 #endif
1792             full_free++;
1793             p = p->next;
1794         }
1795 
1801         pages = full_free * (1<<searchp->gfporder);
    // If the objects have constructors, reduce the page count by one fifth to make is less likely to be selected for reaping. I think this is because the construct is very slow.
    // +1 in the bracket is means rounding to the closest number.
    // Like 6*4/5 = 4.8, so (6*4+1)/5=5.
    // 4*4/5=3.2, so (4*4+1)/5 = 3
1802         if (searchp->ctor)
1803             pages = (pages*4+1)/5;
    // If the slabs consist of more than one page, reduce the page count by one fifth. This is because high order pages are hard to acquire.
1804         if (searchp->gfporder)
1805             pages = (pages*4+1)/5;
    // If this is the best candidate found for reaping so far.
1806         if (pages > best_pages) {
1807             best_cachep = searchp;
1808             best_len = full_free;
1809             best_pages = pages;
1810             if (pages >= REAP_PERFECT) {
1811                 clock_searchp =
                      list_entry(searchp->next.next,
1812                      kmem_cache_t,next);
1813                 goto perfect;
1814             }
1815         }
1816 next_unlock:
1817         spin_unlock_irq(&searchp->spinlock);
1818 next:
1819         searchp =
               list_entry(searchp->next.next,kmem_cache_t,next);
1820     } while (--scan && searchp != clock_searchp);

1822     clock_searchp = searchp;
1823 
    // no cache can be reaped
1824     if (!best_cachep)
1826         goto out;
1827 
    // Disable interrupt due to some caches can be used from interrupt context.
1828     spin_lock_irq(&best_cachep->spinlock);
1829 perfect:
1830     /* free only 50% of the free slabs */
1831     best_len = (best_len + 1)/2;
1832     for (scan = 0; scan < best_len; scan++) {
1833         struct list_head *p;
1834 
1835         if (best_cachep->growing)
1836             break;
1837         p = best_cachep->slabs_free.prev;
1838         if (p == &best_cachep->slabs_free)
1839             break;
1840         slabp = list_entry(p,slab_t,list);
1841 #if DEBUG
1842         if (slabp->inuse)
1843             BUG();
1844 #endif
1845         list_del(&slabp->list);
1846         STATS_INC_REAPED(best_cachep);
1847 
1848         /* Safe to drop the lock. The slab is no longer 
1849          * lined to the cache.
1850          */
1851         spin_unlock_irq(&best_cachep->spinlock);
1852         kmem_slab_destroy(best_cachep, slabp);
1853         spin_lock_irq(&best_cachep->spinlock);
1854     }
1855     spin_unlock_irq(&best_cachep->spinlock);
1856     ret = scan * (1 << best_cachep->gfporder);
1857 out:
1858     up(&cache_chain_sem);
1859     return ret;
1860 }
```



### 2. Slabs

#### 2.1 Storing the Slab Descriptor

##### 2.1.1 Function: kmem_cache_slabmgmt (mm/slab.c)

```C
/*
 * @cachep: The cache the slab is to be allocated to.
 * @objp: This points to the beginning of the slab
 * @coulour_off: The colour offset for this slab
 * @local_flags: These are the flags for the cache.
 */
1032 static inline slab_t * kmem_cache_slabmgmt (
                 kmem_cache_t *cachep,
1033             void *objp, 
                 int colour_off, 
                 int local_flags)
1034 {
1035     slab_t *slabp;
1036     
    // The OFF_SLAB() is true means the slab descriptor is at another place.
    // it is false means the slab descriptor is locate at the head of the slab page.
1037     if (OFF_SLAB(cachep)) {
1039         slabp = kmem_cache_alloc(cachep->slabp_cache,
                          local_flags);
1040         if (!slabp)
1041             return NULL;
1042     } else {
    // We will put the slab descriptor at the (objp + colour_off). objp is the beginning of the slab.
1047         slabp = objp+colour_off;
    // colour_off is calculated to be the offset where the first object will be placed.
1048         colour_off += L1_CACHE_ALIGN(cachep->num *
1049                 sizeof(kmem_bufctl_t) + 
                     sizeof(slab_t));
1050     }
1051     slabp->inuse = 0;
1052     slabp->colouroff = colour_off;
    // s_mem is the first object will be allocated.
1053     slabp->s_mem = objp+colour_off;
1054 
1055     return slabp;
1056 }
```

##### 2.1.2 Function: kmem_find_general_cachep (mm/slab.c)

```C
1620 kmem_cache_t * kmem_find_general_cachep (size_t size, 
                          int gfpflags)
1621 {
1622     cache_sizes_t *csizep = cache_sizes;
1623 
1628     for ( ; csizep->cs_size; csizep++) {
1629         if (size > csizep->cs_size)
1630             continue;
1631         break;
1632     }
1633     return (gfpflags & GFP_DMA) ? csizep->cs_dmacachep :
                       csizep->cs_cachep;
1634 }
```

#### 2.2 Slab Creation

##### 2.2.1 Function: kmem_cache_grow (mm/slab.c)

```C
/*
 * @cachep: The cache to allocate a new slab to
 * @flags: The flags for a slab creation
 */
1105 static int kmem_cache_grow (kmem_cache_t * cachep, int flags)
1106 {
1107     slab_t  *slabp;
1108     struct page     *page;
1109     void        *objp;
1110     size_t       offset;
1111     unsigned int     i, local_flags;
1112     unsigned long    ctor_flags;
1113     unsigned long    save_flags;

1118     if (flags & ~(SLAB_DMA|SLAB_LEVEL_MASK|SLAB_NO_GROW))
1119         BUG();
1120     if (flags & SLAB_NO_GROW)
1121         return 0;
1122 
1129     if (in_interrupt() && 
             (flags & SLAB_LEVEL_MASK) != SLAB_ATOMIC)
1130         BUG();
1131 
1132     ctor_flags = SLAB_CTOR_CONSTRUCTOR;
1133     local_flags = (flags & SLAB_LEVEL_MASK);
1134     if (local_flags == SLAB_ATOMIC)
1139         ctor_flags |= SLAB_CTOR_ATOMIC;

    // Acquire an interrupt safe lock for accessing the cache descriptor
1142     spin_lock_irqsave(&cachep->spinlock, save_flags);
1143 
1145     offset = cachep->colour_next;
1146     cachep->colour_next++;
1147     if (cachep->colour_next >= cachep->colour)
1148         cachep->colour_next = 0;
1149     offset *= cachep->colour_off;
    // Mark the cache that it is growing so that kmem_cache_reap() will ignore this cache.
1150     cachep->dflags |= DFLGS_GROWN;
1151 
1152     cachep->growing++;
1153     spin_unlock_irqrestore(&cachep->spinlock, save_flags);
    
    // Allocate pages from the page allocator for the slab
1165     if (!(objp = kmem_getpages(cachep, flags)))
1166         goto failed;
1167 
    // Acquire a slab descriptor
1169     if (!(slabp = kmem_cache_slabmgmt(cachep, 
                           objp, offset,
                           local_flags)))
1160         goto opps1;

1173     i = 1 << cachep->gfporder;
1174     page = virt_to_page(objp);
1175     do {
    // Link each pages list field to the slab and cache descriptors
    // SET_PAGE_CACHE() links the page to the cache descriptor using the page->list.next field
1176         SET_PAGE_CACHE(page, cachep);
    // SET_PAGE_SLAB() links the page to the slab descriptor using the page->list.prev field.
1177         SET_PAGE_SLAB(page, slabp);
1178         PageSetSlab(page);
1179         page++;
1180     } while (--i);
    
    // Initialise all objects
1182     kmem_cache_init_objs(cachep, slabp, ctor_flags);
    
1184     spin_lock_irqsave(&cachep->spinlock, save_flags);
1185     cachep->growing--;
1186 
    // Add the slab to the end of the slabs_free list
1188     list_add_tail(&slabp->list, &cachep->slabs_free);
1189     STATS_INC_GROWN(cachep);
1190     cachep->failures = 0;
1191 
1192     spin_unlock_irqrestore(&cachep->spinlock, save_flags);
    // Return success
1193     return 1;

1194 opps1:
1195     kmem_freepages(cachep, objp);
1196 failed:
1197     spin_lock_irqsave(&cachep->spinlock, save_flags);
1198     cachep->growing--;
1199     spin_unlock_irqrestore(&cachep->spinlock, save_flags);
1300     return 0;
1301 }
```

#### 2.3 Slab Destroying

##### 2.3.1 Function: kmem_slab_destroy (mm/slab.c)

This function is been used for destroy a slab. It's very simple.

```C
555 static void kmem_slab_destroy (kmem_cache_t *cachep, slab_t *slabp)
556 {
    // If it has desctructor, call it for every object.
557     if (cachep->dtor
561     ) {
562         int i;
563         for (i = 0; i < cachep->num; i++) {
564             void* objp = slabp->s_mem+cachep->objsize*i;

565-574 DEBUG: Check red zone markers

575             if (cachep->dtor)
576                 (cachep->dtor)(objp, cachep, 0);

577-584 DEBUG: Check poison pattern

585         }
586     }
587 
    // Free the slab pages to buddy
588     kmem_freepages(cachep, slabp->s_mem-slabp->colouroff);
    // If it's off slab, free the slab descriptor.
589     if (OFF_SLAB(cachep))
590         kmem_cache_free(cachep->slabp_cache, slabp);
591 }
```

### 3. Objects

This section covers how objects are managed.

#### 3.1 Initializing Objects in a Slab

##### 3.1.1 Function: kmem_cache_init_objs (mm/slab.c)

```C
1058 static inline void kmem_cache_init_objs (kmem_cache_t * cachep,
1059             slab_t * slabp, unsigned long ctor_flags)
1060 {
1061     int i;
1062 
1063     for (i = 0; i < cachep->num; i++) {
1064         void* objp = slabp->s_mem+cachep->objsize*i;

1065-1072        /* Debugging Part 1 */

1079         if (cachep->ctor)
1080             cachep->ctor(objp, cachep, ctor_flags);

1081-1094        /* Debugging Part 2 */
	// The macro slab_bufctl() is defined as:
    // ((kmem_bufctl_t *)(((slab_t*)slabp)+1))
1095         slab_bufctl(slabp)[i] = i+1;
1096     }
1097     slab_bufctl(slabp)[i-1] = BUFCTL_END;
1098     slabp->free = 0;
1099 }
```

#### 3.2 Object Allocation

#### 3.2.1 Function: kmem_cache_alloc (mm/slab.c) (UP Case)

UP is means Unique Processor. One processor is more precise.

```C
1529 void * kmem_cache_alloc (kmem_cache_t *cachep, int flags)
1531 {
1532     return __kmem_cache_alloc(cachep, flags);
1533 }
```

#### Function: __kmem_cache_alloc (mm/slab.c)

```C
1338 static inline void * __kmem_cache_alloc (kmem_cache_t *cachep, 
                                              int flags)
1339 {
1340     unsigned long save_flags;
1341     void* objp;
1342 
1343     kmem_cache_alloc_head(cachep, flags);
1344 try_again:
1345     local_irq_save(save_flags);

    // This is a macro, if allocate ok, return objp.
    // If allocate failed, goto alloc_new_slab
1367     objp = kmem_cache_alloc_one(cachep);

1369     local_irq_restore(save_flags);
1370     return objp;
1371 alloc_new_slab:

1376     local_irq_restore(save_flags);
1377     if (kmem_cache_grow(cachep, flags))
1381         goto try_again;
1382     return NULL;
1383 }
```

##### 3.2.3 Function: __kmem_cache_alloc (mm/slab.c) (SMP Case)

```C
1338 static inline void * __kmem_cache_alloc (kmem_cache_t *cachep, 
                                              int flags)
1339 {
1340     unsigned long save_flags;
1341     void* objp;
1342 
    // This function makes sure the appropriate combination of DMA flags are in use
1343     kmem_cache_alloc_head(cachep, flags);
1344 try_again:
1345     local_irq_save(save_flags);
1347     {
    // #define cc_data(cachep) \
	//	((cachep)->cpudata[smp_processor_id()])
    // It will get the percpu data stored in the cachep.
1348         cpucache_t *cc = cc_data(cachep);
1349 
1350         if (cc) {
1351             if (cc->avail) {
        // If there is an object available
1352                 STATS_INC_ALLOCHIT(cachep);
1353                 objp = cc_entry(cc)[--cc->avail];
1354             } else {
1355                 STATS_INC_ALLOCMISS(cachep);
1356                 objp =
                  kmem_cache_alloc_batch(cachep,cc,flags);
1357                 if (!objp)
1358                   goto alloc_new_slab_nolock;
1359             }
1360         } else {
1361             spin_lock(&cachep->spinlock);
1362             objp = kmem_cache_alloc_one(cachep);
1363             spin_unlock(&cachep->spinlock);
1364         }
1365     }
1366     local_irq_restore(save_flags);
1370     return objp;
1371 alloc_new_slab:
1373     spin_unlock(&cachep->spinlock);
1374 alloc_new_slab_nolock:
1375     local_irq_restore(save_flags);
1377     if (kmem_cache_grow(cachep, flags))
1381         goto try_again;
1382     return NULL;
1383 }
```

##### 3.2.4 Function: kmem_cache_alloc_head (mm/slab.c)

This simple function ensures the right combination of slab and GFP flags are used for allocation from a slab.

```C
1231 static inline void kmem_cache_alloc_head(kmem_cache_t *cachep, 
                                              int flags)
1232 {
1233     if (flags & SLAB_DMA) {
1234         if (!(cachep->gfpflags & GFP_DMA))
1235             BUG();
1236     } else {
1237         if (cachep->gfpflags & GFP_DMA)
1238             BUG();
1239     }
1240 }
```

##### 3.2.5 Function: kmem_cache_alloc_one (mm/slab.c)

```C
1283 #define kmem_cache_alloc_one(cachep)              \
1284 ({                                                \ 
1285     struct list_head * slabs_partial, * entry;    \
1286     slab_t *slabp;                                \
1287                                                   \
1288     slabs_partial = &(cachep)->slabs_partial;     \
1289     entry = slabs_partial->next;                  \
1290     if (unlikely(entry == slabs_partial)) {       \
1291         struct list_head * slabs_free;            \
1292         slabs_free = &(cachep)->slabs_free;       \
1293         entry = slabs_free->next;                 \
1294         if (unlikely(entry == slabs_free))        \
1295             goto alloc_new_slab;                  \
1296         list_del(entry);                          \
1297         list_add(entry, slabs_partial);           \
1298     }                                             \
1299                                                   \
1300     slabp = list_entry(entry, slab_t, list);      \
1301     kmem_cache_alloc_one_tail(cachep, slabp);     \
1302 })
```

##### 3.2.6 Function: kmem_cache_alloc_one_tail (mm/slab.c)

This function is responsible for the allocation of one object from a slab.

```C
1242 static inline void * kmem_cache_alloc_one_tail (
                             kmem_cache_t *cachep,
1243                         slab_t *slabp)
1244 {
1245     void *objp;
1246 
1247     STATS_INC_ALLOCED(cachep);
1248     STATS_INC_ACTIVE(cachep);
1249     STATS_SET_HIGH(cachep);
1250 
    // inuse is the number of objects active on this slab
1252     slabp->inuse++;
    // Get a pointer to a free object
1253     objp = slabp->s_mem + slabp->free*cachep->objsize;
    // This updates the free pointer to be an index of the next free object
1254     slabp->free=slab_bufctl(slabp)[slabp->free];
1255 
    // If the slab is full, remove it from the slabs_partial and place it on the slabs_full.
1256     if (unlikely(slabp->free == BUFCTL_END)) {
1257         list_del(&slabp->list);
1258         list_add(&slabp->list, &cachep->slabs_full);
1259     }
1260 #if DEBUG
1261     if (cachep->flags & SLAB_POISON)
1262         if (kmem_check_poison_obj(cachep, objp))
1263             BUG();
1264     if (cachep->flags & SLAB_RED_ZONE) {
1266         if (xchg((unsigned long *)objp, RED_MAGIC2) !=
1267                           RED_MAGIC1)
1268             BUG();
1269         if (xchg((unsigned long *)(objp+cachep->objsize -
1270             BYTES_PER_WORD), RED_MAGIC2) != RED_MAGIC1)
1271             BUG();
1272         objp += BYTES_PER_WORD;
1273     }
1274 #endif
1275     return objp;
1276 }
```

##### 3.2.7 Function: kmem_cache_alloc_batch (mm/slab.c)

This function allocate a batch of objects to a CPU cache of objects. It is only used in SMP case.

```C
// @cc: is the per CPU cache.
1305 void* kmem_cache_alloc_batch(kmem_cache_t* cachep, 
                  cpucache_t* cc, int flags)
1306 {
    // The number of objects to allocate
1307     int batchcount = cachep->batchcount;
1308 
1309     spin_lock(&cachep->spinlock);
1310     while (batchcount--) {
1311         struct list_head * slabs_partial, * entry;
1312         slab_t *slabp;
1313         /* Get slab alloc is to come from. */
1314         slabs_partial = &(cachep)->slabs_partial;
1315         entry = slabs_partial->next;
1316         if (unlikely(entry == slabs_partial)) {
1317             struct list_head * slabs_free;
1318             slabs_free = &(cachep)->slabs_free;
1319             entry = slabs_free->next;
1320             if (unlikely(entry == slabs_free))
1321                 break;
1322             list_del(entry);
1323             list_add(entry, slabs_partial);
1324         }
1325 
1326         slabp = list_entry(entry, slab_t, list);
1327         cc_entry(cc)[cc->avail++] =
1328                kmem_cache_alloc_one_tail(cachep, slabp);
1329     }
1330     spin_unlock(&cachep->spinlock);
1331 
1332     if (cc->avail)
1333         return cc_entry(cc)[--cc->avail];
1334     return NULL;
1335 }
```

#### 3.3 Object Freeing

##### 3.3.1 Function: kmem_cache_free (mm/slab.c)

```C
1576 void kmem_cache_free (kmem_cache_t *cachep, void *objp)
1577 {
1578     unsigned long flags;
1579 #if DEBUG
1580     CHECK_PAGE(virt_to_page(objp));
1581     if (cachep != GET_PAGE_CACHE(virt_to_page(objp)))
1582         BUG();
1583 #endif
1584 
1585     local_irq_save(flags);
    // __kmem_cache_free() will free the object to the per-CPU cache for the SMP case and to the global pool in the normal case.
1586     __kmem_cache_free(cachep, objp);
1587     local_irq_restore(flags);
1588 }
```

##### 3.3.2 Function: __kmem_cache_free(mm/slab.c) (UP case)

```C
1493 static inline void __kmem_cache_free (kmem_cache_t *cachep, 
                                           void* objp)
1494 {
1517     kmem_cache_free_one(cachep, objp);
1519 }
```

##### 3.3.3 Function: __kmem_cache_free(mm/slab.c) (SMP case)

```C
1493 static inline void __kmem_cache_free (kmem_cache_t *cachep, 
                                          void* objp)
1494 {
1496     cpucache_t *cc = cc_data(cachep);
1497 
    // Make sure the page is a slab page.
1498     CHECK_PAGE(virt_to_page(objp));
1499     if (cc) {
1500         int batchcount;
    // If the number of available in the per CPU cache is below limit, then add the object to the free list and return
1501         if (cc->avail < cc->limit) {
1502             STATS_INC_FREEHIT(cachep);
1503             cc_entry(cc)[cc->avail++] = objp;
1504             return;
1505         }
1506         STATS_INC_FREEMISS(cachep);
    // The pool has overflowed so batchcount number of objects is going to be freed to the global pool.
1507         batchcount = cachep->batchcount;
1508         cc->avail -= batchcount;
1509         free_block(cachep,
1510             &cc_entry(cc)[cc->avail],batchcount);
1511         cc_entry(cc)[cc->avail++] = objp;
1512         return;
1513     } else {
1514         free_block(cachep, &objp, 1);
1515     }
1519 }
```

##### 3.3.4 Function: kmem_cache_free_one (mm/slab.c)

```C
1414 static inline void kmem_cache_free_one(kmem_cache_t *cachep, 
                                            void *objp)
1415 {
1416     slab_t* slabp;
1417 
1418     CHECK_PAGE(virt_to_page(objp));
1425     slabp = GET_PAGE_SLAB(virt_to_page(objp));
1426 
1427 #if DEBUG
1428     if (cachep->flags & SLAB_DEBUG_INITIAL)
1433         cachep->ctor(objp, cachep,
            SLAB_CTOR_CONSTRUCTOR|SLAB_CTOR_VERIFY);
1434 
1435     if (cachep->flags & SLAB_RED_ZONE) {
1436         objp -= BYTES_PER_WORD;
1437         if (xchg((unsigned long *)objp, RED_MAGIC1) !=
                             RED_MAGIC2)
1438             BUG();
1440         if (xchg((unsigned long *)(objp+cachep->objsize -
1441                 BYTES_PER_WORD), RED_MAGIC1) !=
                              RED_MAGIC2)
1443             BUG();
1444     }
1445     if (cachep->flags & SLAB_POISON)
1446         kmem_poison_obj(cachep, objp);
1447     if (kmem_extra_free_checks(cachep, slabp, objp))
1448         return;
1449 #endif
1450     {
    // Calculate the index of the object which will be freed.
1451         unsigned int objnr = (objp-slabp->s_mem)/cachep->objsize;
1452 
    // Update the slab_bufctl, which is the free list but stored with array
1453         slab_bufctl(slabp)[objnr] = slabp->free;
1454         slabp->free = objnr;
1455     }
1456     STATS_DEC_ACTIVE(cachep);
1457     
1459     {
1460         int inuse = slabp->inuse;
1461         if (unlikely(!--slabp->inuse)) {
1462             /* Was partial or full, now empty. */
    // All the object in current slab is free, so move it to slabs_free list.
1463             list_del(&slabp->list);
1464             list_add(&slabp->list, &cachep->slabs_free);
1465         } else if (unlikely(inuse == cachep->num)) {
1466             /* Was full. */
    // Original the slab is full, but now partial.
1467             list_del(&slabp->list);
1468             list_add(&slabp->list, &cachep->slabs_partial);
1469         }
1470     }
1471 }
```

##### 3.3.5 Function: free_block (mm/slab.c)

This function is only used in the SMP case when the per CPU cache gets too full. It is used to free a batch of objects in bulk.

```C
// @objpp: Pointer to the first object to free.
// @len: the number of objects to free.
1481 static void free_block (kmem_cache_t* cachep, void** objpp, 
                             int len)
1482 {
1483     spin_lock(&cachep->spinlock);
1484     __free_block(cachep, objpp, len);
1485     spin_unlock(&cachep->spinlock);
1486 }
```

##### 3.3.6 Function: __free_block (mm/slab.c)

```C
1474 static inline void __free_block (kmem_cache_t* cachep,
1475                 void** objpp, int len)
1476 {
    // free every object provided by objpp.
1477     for ( ; len > 0; len--, objpp++)
1478         kmem_cache_free_one(cachep, *objpp);
1479 }
```

### 4. Sizes Cache

