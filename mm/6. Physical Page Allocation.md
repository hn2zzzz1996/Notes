# 6. Physical Page Allocation

## Function

### Physical Page Allocation Initialize

Once the system is started, the boot memory allocator is no longer needed, so it will pass the remaining pages to the normal physical allocator.

#### 6.1 Function: mem_init (arch/i386/mm/init.c)

```C
507 void __init mem_init(void)
508 {
509     int codesize, reservedpages, datasize, initsize;
510 
511     if (!mem_map)
512         BUG();
513     
514     set_max_mapnr_init();
515 
516     high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
517 
518     /* clear the zero-page */
519     memset(empty_zero_page, 0, PAGE_SIZE);
520 
521     reservedpages = free_pages_init();
522 
    ...
```

#### 6.2 Function: free_pages_init (arch/i386/mm/init.c)

```C
481 static int __init free_pages_init(void)
482 {
483     extern int ppro_with_ram_bug(void);
484     int bad_ppro, reservedpages, pfn;
487 
488     /* this will put all low memory onto the freelists */
489     totalram_pages += free_all_bootmem();
490 
491     reservedpages = 0;
492     for (pfn = 0; pfn < max_low_pfn; pfn++) {
493         /*
494          * Only count reserved RAM pages
495          */
496         if (page_is_ram(pfn) && PageReserved(mem_map+pfn))
497             reservedpages++;
498     }
499 #ifdef CONFIG_HIGHMEM
500     for (pfn = highend_pfn-1; pfn >= highstart_pfn; pfn--)
501         one_highpage_init((struct page *) (mem_map + pfn), pfn,
bad_ppro);
502     totalram_pages += totalhigh_pages;
503 #endif
504     return reservedpages;
505 }
```

#### 6.4 Function: free_all_bootmem (mm/bootmem.c)

```C
299 unsigned long __init free_all_bootmem_node (pg_data_t *pgdat)
300 {
301     return(free_all_bootmem_core(pgdat));
302 }

321 unsigned long __init free_all_bootmem (void)
322 {
323     return(free_all_bootmem_core(&contig_page_data));
324 }
```

#### 6.5 Function: free_all_bootmem_core (mm/bootmem.c)

This function divide into two major tasks:

* For all unallocated pages known to the allocator for this node;
  * Clear the PG_reserved flag in its struct page
  * Set the count to 1
  * Call __free_pages() so that the buddy allocator can build its free lists.
* Free all pages used for the bitmap and free to them to the buddy allocator.

```C
245 static unsigned long __init free_all_bootmem_core(pg_data_t *pgdat)
246 {
247     struct page *page = pgdat->node_mem_map;
248     bootmem_data_t *bdata = pgdat->bdata;
249     unsigned long i, count, total = 0;
250     unsigned long idx;
251 
    // If no map is available, it means that this node has already been freed and something is wrong.
252     if (!bdata->node_bootmem_map) BUG();
253 
254     count = 0;
255     idx = bdata->node_low_pfn - 
              (bdata->node_boot_start >> PAGE_SHIFT);
256     for (i = 0; i < idx; i++, page++) {
257         if (!test_bit(i, bdata->node_bootmem_map)) {
258             count++;
259             ClearPageReserved(page);
260             set_page_count(page, 1);
261             __free_page(page);
262         }
263     }
264     total += count;
270     page = virt_to_page(bdata->node_bootmem_map);
271     count = 0;
272     for (i = 0; 
        i < ((bdata->node_low_pfn - (bdata->node_boot_start >> PAGE_SHIFT)
                          )/8 + PAGE_SIZE-1)/PAGE_SIZE; 
        i++,page++) {
273         count++;
274         ClearPageReserved(page);
275         set_page_count(page, 1);
276         __free_page(page);
277     }
278     total += count;
279     bdata->node_bootmem_map = NULL;
280 
281     return total;
282 }
```

### 1. Allocating Pages

#### 1.1 Function: alloc_pages (include/linux/mm.h)

```C
// The gfp_mask flags tells the allocator how it may behave. For example GFP_WAIT is not set, the allocator will not block and instead return NULL if memory is tight.
439 static inline struct page * alloc_pages(unsigned int gfp_mask, 
                                  unsigned int order)
440 {
444       if (order >= MAX_ORDER)
445             return NULL;
446       return _alloc_pages(gfp_mask, order);
447 }
```

#### 1.2 Function: _alloc_pages (mm/page_alloc.c)

This is the UMA version. The NUMA version is in the `mm/numa.c`.

```C
244 #ifndef CONFIG_DISCONTIGMEM
245 struct page *_alloc_pages(unsigned int gfp_mask, 
                              unsigned int order)
246 {
    // node_zonelists is an array of preferred fallback zones to allocate from. It is initialised in build_zonelists().
247     return __alloc_pages(gfp_mask, order,
248       contig_page_data.node_zonelists+(gfp_mask & GFP_ZONEMASK));
249 }
250 #endif
```

#### 1.3 Function: __alloc_pages (mm/page_alloc.c)

It is responsible for cycling through the fallback zones and selecting one suitable for the allocation. If memory is tight, it will take some steps to address the problem. It will wake `kswapd` and it will do the work of `kswapd` manually.

```C
327 struct page * __alloc_pages(unsigned int gfp_mask, 
                                unsigned int order,
                                zonelist_t *zonelist)
328 {
329       unsigned long min;
330       zone_t **zone, * classzone;
331       struct page * page;
332       int freed;
333 
334       zone = zonelist->zones;
    // The preferred zone is recorded as the classzone.
335       classzone = *zone;
336       if (classzone == NULL)
337             return NULL;
338       min = 1UL << order;
339       for (;;) {
340             zone_t *z = *(zone++);
341             if (!z)
342                   break;
343 
344             min += z->pages_low;
    // Actually, it equals to (z->free_pages - z->pages_low > min)
345             if (z->free_pages > min) {
346                   page = rmqueue(z, order);
347                   if (page)
348                         return page;
349             }
350       }
    
    // Can't alloc pages, Mark the preferred zone as needing balance. This flag will be read later by kswapd.
352       classzone->need_balance = 1;
    // The memory barrier ensures all CPU's will see any changes made to variables before this line of code. This is important because kswapd could be running on a different processor to the memory allocator.
353       mb();
    // Wake up kswapd if it is asleep
354       if (waitqueue_active(&kswapd_wait))
355             wake_up_interruptible(&kswapd_wait);
356 
357       zone = zonelist->zones;
358       min = 1UL << order;
    // Cycle through all the zones again. This time, allocate the pages if they can be allocated without hitting the pages_min watermark.
359       for (;;) {
360             unsigned long local_min;
361             zone_t *z = *(zone++);
362             if (!z)
363                   break;
364 
365             local_min = z->pages_min;
366             if (!(gfp_mask & __GFP_WAIT))
367                   local_min >>= 2;
368             min += local_min;
369             if (z->free_pages > min) {
370                   page = rmqueue(z, order);
371                   if (page)
372                         return page;
373             }
374       }
375 
376       /* here we're in the low on memory slow path */
377 
378 rebalance:
379       if (current->flags & (PF_MEMALLOC | PF_MEMDIE)) {
380             zone = zonelist->zones;
381             for (;;) {
382                   zone_t *z = *(zone++);
383                   if (!z)
384                         break;
385 
386                   page = rmqueue(z, order);
387                   if (page)
388                         return page;
389             }
390             return NULL;
391       }
    
393       /* Atomic allocations - we can't balance anything */
394       if (!(gfp_mask & __GFP_WAIT))
395             return NULL;
396 
    // balance_classzone() performs the work of kswapd in a synchronous fashion. The principal difference is that instead of freeing the memory into a global pool, it is kept for the process using the current->local_pages linked list.
397       page = balance_classzone(classzone, gfp_mask, order, &freed);
    // If a page block of the right order has been freed, return it. If this is NULL, it doesn't mean an allocation is failed due to it could be a higher order of pages that was released.
398       if (page)
399             return page;
400 
401       zone = zonelist->zones;
402       min = 1UL << order;
403       for (;;) {
404             zone_t *z = *(zone++);
405             if (!z)
406                   break;
407 
408             min += z->pages_min;
409             if (z->free_pages > min) {
410                   page = rmqueue(z, order);
411                   if (page)
412                         return page;
413             }
414       }
415 
416       /* Don't let big-order allocations loop */
417       if (order > 3)
418             return NULL;
419 
420       /* Yield for kswapd, and try again */
421       yield();
422       goto rebalance;
423 }
```

#### 1.4 Function: rmqueue (mm/page_alloc.c)

