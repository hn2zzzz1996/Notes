# 10. Page Frame Reclamation

All data that is read from disk is stored in the `page cache` to reduce the amount of disk IO that must be performed.

## 1. Page Replacement Policy

This policy is frequently said `Last Recently Used (LRU)`. The LRU in Linux consists of two lists called the `active_list` and `inactive_list`. 

* The `active_list` contain the `working set` of all process.
* The `inactive_list` contain reclaim candidates.

All process pages will be added to the LRU list. But how can these pages be moved from `inactive_list` to `active_list`, or moved from `active_list` to `inactive_list`?

* `inactive_list` to `active_list`: When the `try_to_swap_out` function being called from kswapd, it will check if the page has been accessed. It's rely on the `ACCESSED_BIT` in the page table which will be set by cpu when it access the page. If the page has been accessed recently, it will be `mark_page_accessed()` which will make it move from `inactive_list` to `active_list`.
* `active_list` to `inactive_list`: This is been handled by `refill_inactive`, which will be called by `shrink_caches`, which is called by kswapd.

## 2. Page Cache

The page cache is a set of data structures which contain pages that are backed by regular files, block devices or swap. There are basically four types of pages that exist in the cache:

* Pages that were faulted in as a result of reading a memory mapped file;
* Buffer pages that used for reading block device or filesystem.
* Anonymous pages exist in the `swap cache` when slots are allocated in the backing storage for page-out.
* Pages belonging to shared memory regions. It's similar to anonymous pages. The only difference is that shared pages are added to the swap cache and space in backing storage immediately after the first write to the page.

The principal reason for the existence of this cache is to `eliminate unnecessary disk reads`. Pages read from disk are stored in a `page hash` table which is hashed on the `struct address_space` and the offset which is always searched before the disk is accessed.

### 2.1 Adding Pages to the Page Cache

Pages read from a file or block device are generally added to the page cache to avoid further disk IO. Most filesystems use the high level function `generic_file_read()` as their `file_operations`→`read()`. 

For normal IO, `generic_file_read()` first searches the page cache, by calling `__find_page_nolock()` with the `pagecache_lock` held, to see if the page already exists in it. If not exist, a new page is allocated, and added to the page cache with `__add_to_page_cache()`. Once a page frame is present in the page cache, `generic_file_readahead()` is called which uses `page_cache_read()` to read the page from disk. It reads the page using `mapping->a_ops->readpage()`, where mapping is the address_space managing the file. `read_page()` is the filesystem specific function used to read a page on disk.

## Functions

This section addresses how pages are added and removed from the page cache and LRU lists, both of which are heavily intertwined(交织在一起).

### 1. Page Cache Operations

#### 1.1 Adding Pages to Page Cache

##### 1.1.1 Function: add_to_page_cache() (mm/filemap.c)

```C
void add_to_page_cache(struct page * page, struct address_space * mapping, unsigned long offset)
{
    // Acquires the lock protecting the page hash and inode queues.
	spin_lock(&pagecache_lock);
    // page_hash() hashes into the page hash table based on the mapping and the offset within the file
	__add_to_page_cache(page, mapping, offset, page_hash(mapping, offset));
	spin_unlock(&pagecache_lock);
	lru_cache_add(page);
}
```

##### 1.1.2 add_to_page_cache_unique() (mm/filemap.c)

This function is very similar to `add_to_page_cache()`, except that the function will check the page cache with the pagecache_lock before adding the page to the cache. It is for callers that may race with another process for inserting a page in the cache, such as `add_to_swap_cache()`.

```C
int add_to_page_cache_unique(struct page * page,
	struct address_space *mapping, unsigned long offset,
	struct page **hash)
{
	int err;
	struct page *alias;

	spin_lock(&pagecache_lock);
    // Check if the page already exists in the cache.
	alias = __find_page_nolock(mapping, offset, *hash);

	err = 1;
	if (!alias) {
        // If not in the cache, it's same with add_to_page_cache().
		__add_to_page_cache(page,mapping,offset,hash);
		err = 0;
	}

	spin_unlock(&pagecache_lock);
	if (!err)
		lru_cache_add(page);
	return err;
}
```

##### 1.1.3 Function: __add_to_page_cache() (mm/filemap.c)

This clears all page flags, locks the page, increments the reference count for the page and adds the page to the inode and hash queues.

```C
static inline void __add_to_page_cache(struct page * page,
	struct address_space *mapping, unsigned long offset,
	struct page **hash)
{
	unsigned long flags;

    // Clears all page flags.
	flags = page->flags & ~(1 << PG_uptodate | 1 << PG_error | 1 << PG_dirty | 1 << PG_referenced | 1 << PG_arch_1 | 1 << PG_checked);
    // Locks the page. When this bit set, the page must be locked in memory for disk I/O.
	page->flags = flags | (1 << PG_locked);
    // Takes a reference in case it gets freed.
	page_cache_get(page);
    // The file offset stores in the index field.
	page->index = offset;
    // This links the page via the page->list to the clean_pages list in the address_space and points the page->mapping to the same address_space
	add_page_to_inode_queue(mapping, page);
    // The hash page was returned by `page_hash()`
	add_page_to_hash_queue(page, hash);
}
```

##### 1.1.4 Function: add_page_to_inode_queue() (mm/filemap.c)

This function add the page to the `mapping->clean_pages` due to when this function is called, the page is clean.

```C
static inline void add_page_to_inode_queue(struct address_space *mapping, struct page * page)
{
	struct list_head *head = &mapping->clean_pages;

	mapping->nrpages++;
	list_add(&page->list, head);
	page->mapping = mapping;
}
```

##### 1.1.5 Function: add_page_to_hash_queue() (mm/filemap.c)

This adds page to the top of the hash bucket headed by p. P is the page_hash_table.

```C
static void add_page_to_hash_queue(struct page * page, struct page **p)
{
	struct page *next = *p;

	*p = page;
	page->next_hash = next;
	page->pprev_hash = p;
	if (next)
		next->pprev_hash = &page->next_hash;
	if (page->buffers)
		PAGE_BUG(page);
	atomic_inc(&page_cache_size);
}
```

#### 1.2 Deleting Pages From the Page Cache

##### 1.2.1 Function: remove_inode_page() (mm/filemap.c)

```C
void remove_inode_page(struct page *page)
{
    // The page must be locked when it's in a page cache.
	if (!PageLocked(page))
		PAGE_BUG(page);

	spin_lock(&pagecache_lock);
	__remove_inode_page(page);
	spin_unlock(&pagecache_lock);
}
```

##### 1.2.2 Function: __remove_inode_page() (mm/filemap.c)

Remove a page from the pagecache.

```C
void __remove_inode_page(struct page *page)
{
    // Removes the page from its address_space at page->mapping.
	remove_page_from_inode_queue(page);
	remove_page_from_hash_queue(page);
}
```

##### 1.2.3 Function: remove_page_from_inode_queue() (mm/filemap.c)

```C
static inline void remove_page_from_inode_queue(struct page * page)
{
	struct address_space * mapping = page->mapping;

	if (mapping->a_ops->removepage)
		mapping->a_ops->removepage(page);
	
    // Deletes the page from whatever list it belongs to in the mapping, such as the clean_pages list in most cases or the dirty_pages in rarer cases.
	list_del(&page->list);
	page->mapping = NULL;
	wmb();
	mapping->nrpages--;
}
```

##### 1.2.4 Function: remove_page_from_hash_queue() (mm/filemap.c)

```C
static inline void remove_page_from_hash_queue(struct page * page)
{
	struct page *next = page->next_hash;
	struct page **pprev = page->pprev_hash;

	if (next)
		next->pprev_hash = pprev;
	*pprev = next;
	page->pprev_hash = NULL;
	atomic_dec(&page_cache_size);
}
```

#### 1.3 Acquiring/Releasing Page Cache Pages

##### 1.3.1 Function: page_cache_get() (include/linux/pagemap.h)

```C
#define page_cache_get(x)	get_page(x)
```

A simple call `get_page()`, which uses `atomic_inc()` to increment the page reference count.

##### 1.3.2 Function: page_cache_release() (include/linux/pagemap.h)

```C
#define page_cache_release(x)	__free_page(x)
```

calls __free_page(), which decrement the page count. If the count reaches 0, the page will be freed.

#### 1.4 Searching the Page Cache

##### 1.4.1 Function: find_get_page() (include/linux/pagemap.h)

```C
#define find_get_page(mapping, index) \
	__find_get_page(mapping, index, page_hash(mapping, index))
```

##### 1.4.2 Function: __find_get_page() (mm/filemap.c)

```C
struct page * __find_get_page(struct address_space *mapping,
			      unsigned long offset, struct page **hash)
{
	struct page *page;

	/*
	 * We scan the hash list read-only. Addition to and removal from
	 * the hash-list needs a held write-lock.
	 */
	spin_lock(&pagecache_lock);
	page = __find_page_nolock(mapping, offset, *hash);
	if (page)
		page_cache_get(page);
	spin_unlock(&pagecache_lock);
	return page;
}
```

##### 1.4.3 Function: __find_page_nolock() (mm/filemap.c)

```C
static inline struct page * __find_page_nolock(struct address_space *mapping, unsigned long offset, struct page *page)
{
	goto inside;

	for (;;) {
		page = page->next_hash;
inside:
		if (!page)
			goto not_found;
		if (page->mapping != mapping)
			continue;
		if (page->index == offset)
			break;
	}

not_found:
	return page;
}
```

##### 1.4.4 Function: find_lock_page() (include/linux/pagemap.h)

...

### 2. LRU List Operations

#### 2.1 Adding Pages to the LRU Lists

##### 2.1.1 Function: lru_cache_add() (mm/swap.c)

Add a page to the LRU `inactive_list`.

```C
void lru_cache_add(struct page * page)
{
    // If the page is not already part of the LRU lists, then adds it.
	if (!PageLRU(page)) {
		spin_lock(&pagemap_lru_lock);
        // test and set the LRU bit. And calls add_page_to_inactive_list().
		if (!TestSetPageLRU(page))
			add_page_to_inactive_list(page);
		spin_unlock(&pagemap_lru_lock);
	}
}
```

##### 2.1.2 Function: add_page_to_active_list() (include/linux/swap.h)

This add the page to the `active_list`.

```C
#define add_page_to_active_list(page)		\
do {						\
	DEBUG_LRU_PAGE(page);			\
	// Set the active flag to show it's on the active_list.
	SetPageActive(page);			\
	list_add(&(page)->lru, &active_list);	\
	nr_active_pages++;			\
} while (0)
```

##### 2.1.3 Function: add_page_to_inactive_list() (include/linux/swap.h)

This adds the page to `inactive_list`.

```C
#define add_page_to_inactive_list(page)		\
do {						\
	DEBUG_LRU_PAGE(page);			\
	list_add(&(page)->lru, &inactive_list);	\
	nr_inactive_pages++;			\
} while (0)
```

#### 2.2 Deleting Pages From the LRU lists

##### 2.2.1 Function: lru_cache_del() (mm/swap.c)

```C
 90 void lru_cache_del(struct page * page)
 91 {
 92       spin_lock(&pagemap_lru_lock);
 93       __lru_cache_del(page);
 94       spin_unlock(&pagemap_lru_lock);
 95 }
```

##### 2.2.2 Function: __lru_cache_del (mm/swap.c)

```C
 75 void __lru_cache_del(struct page * page)
 76 {
     // Clear the flags, and do the real work in the right list.
 77       if (TestClearPageLRU(page)) {
 78             if (PageActive(page)) {
 79                   del_page_from_active_list(page);
 80             } else {
 81                   del_page_from_inactive_list(page);
 82             }
 83       }
 84 }
```

##### 2.2.3 Function: del_page_from_active_list (include/linux/swap.h)

```C
193 #define del_page_from_active_list(page)   \
194 do {                                      \
195       list_del(&(page)->lru);             \
196       ClearPageActive(page);              \
197       nr_active_pages--;                  \
198 } while (0)
```

##### 2.2.4 Function: del_page_from_inactive_list (include/linux/swap.h)

```C
200 #define del_page_from_inactive_list(page) \
201 do {                                      \
202       list_del(&(page)->lru);             \
203       nr_inactive_pages--;                \
204 } while (0)
```

#### 2.3 Activating Pages

##### 2.3.1 Function: mark_page_accessed (mm/filemap.c)

This marks that a page has been referenced. If the page is already on the `active_list` or the referenced flag is clear, the referenced flag will be simply set.

```C
1332 void mark_page_accessed(struct page *page)
1333 {
1334       if (!PageActive(page) && PageReferenced(page)) {
1335             activate_page(page);
1336             ClearPageReferenced(page);
1337       } else
1338             SetPageReferenced(page);
1339 }
```

If the page is on the `inactive_list(!PageActive())` and has been referenced recently(PageReferenced()), activate_page() is called to move it to the `active_list`.

Otherwise, just simply mark the page as referenced.

##### 2.3.2 Function: activate_page (mm/swap.c)

```C
 47 void activate_page(struct page * page)
 48 {
 49       spin_lock(&pagemap_lru_lock);
 50       activate_page_nolock(page);
 51       spin_unlock(&pagemap_lru_lock);
 52 }
```

##### 2.3.3 Function: activate_page_nolock (mm/swap.c)

Move the page from the `inactive_list` to the `active_list`.

```C
39 static inline void activate_page_nolock(struct page * page)
 40 {
 41       if (PageLRU(page) && !PageActive(page)) {
 42             del_page_from_inactive_list(page);
 43             add_page_to_active_list(page);
 44       }
 45 }
```

### 3. Refilling inactive_list

This section covers how pages are moved from the active lists to the inactive lists.

#### 3.1 Function: refill_inactive (mm/vmscan.c)

Move `nr_pages` from the `active_list` to the `inactive_list.`  The parameter `nr_pages` is calculated by `shrink_caches()` and is a number which tries to keep the active list two thirds the size of the page cache.

```C
533 static void refill_inactive(int nr_pages)
534 {
535       struct list_head * entry;
536 
537       spin_lock(&pagemap_lru_lock);
538       entry = active_list.prev;
539       while (nr_pages && entry != &active_list) {
540             struct page * page;
541 
542             page = list_entry(entry, struct page, lru);
543             entry = entry->prev;
    // If this page has been referenced, Moved back to the top of the active_list
544             if (PageTestandClearReferenced(page)) {
545                   list_del(&page->lru);
546                   list_add(&page->lru, &active_list);
547                   continue;
548             }
549 
550             nr_pages--;
551 
    // Move one page from the active_list to the inactive_lsit
552             del_page_from_active_list(page);
553             add_page_to_inactive_list(page);
    // Make it referenced so that if it is referenced again soon, it will be promoted back to the active_list without requiring a second reference
554             SetPageReferenced(page);
555       }
556       spin_unlock(&pagemap_lru_lock);
557 }
```

### 4. Reclaiming Pages from the LRU Lists

This section covers how a page is reclaimed once it has been selected for pageout.

#### 4.1 Function: shrink_cache (mm/vmscan.c)

```C
/*
 * @nr_pages: The number of pages to swap out. nr_pages starts as SWAP_CLUSTER_MAX, which is 32.
 * @classzone: The zone we are interested in swapping pages out for. Pages not belonging to this zone are skipped.
 * @gfp_mask: The gfp determining what actions may be taken such as if filesystem operations may be performed.
 * @priority: The priority of the function. starts at DEF_PRIORITY(6) and decreases to the highest priority of 1.
 */
338 static int shrink_cache(int nr_pages, zone_t * classzone, 
                            unsigned int gfp_mask, int priority)
339 {
340     struct list_head * entry;
    // The maximum number of pages to scan is the number of pages in the inactive_list divided by the priority. At lowest priority, 1/6th of the list may scanned. At highest priority, the full list may be scanned.
341     int max_scan = nr_inactive_pages / priority;
    // The maximum amount of process mapped paged allowed.
342     int max_mapped = min((nr_pages << (10 - priority)), 
                             max_scan / 10);
343 
344     spin_lock(&pagemap_lru_lock);
    // Keep scanning until max_scan pages have been scanned or the inactive_list is empty.
345     while (--max_scan >= 0 && 
               (entry = inactive_list.prev) != &inactive_list) {
346         struct page * page;
347 
348         if (unlikely(current->need_resched)) {
349             spin_unlock(&pagemap_lru_lock);
350             __set_current_state(TASK_RUNNING);
351             schedule();
352             spin_lock(&pagemap_lru_lock);
    // When this process sleep, another process may modify the entry we own now, so reiterate through the loop and take an entry again.
353             continue;
354         }
355 
356         page = list_entry(entry, struct page, lru);
357 
358         BUG_ON(!PageLRU(page));
359         BUG_ON(PageActive(page));
360 
    // Move the page to the top of the inactive_list so that if the page is not freed, we can simpliy continue.
361         list_del(entry);
362         list_add(entry, &inactive_list);
363 
364         /*
365          * Zero page counts can happen because we unlink the pages
366          * _after_ decrementing the usage count..
367          */
    // If the page count has already reached 0, skip over it.
    // This will happen because in __free_pages(), it will first drop the page count before __free_pages_ok() is called to free it. 
368         if (unlikely(!page_count(page)))
369             continue;
370 
371         if (!memclass(page_zone(page), classzone))
372             continue;
373 
374         /* Racy check to avoid trylocking when not worthwhile */
    // This page is mapped by process.
375         if (!page->buffers && (page_count(page) != 1 || !page->mapping))
376             goto page_mapped;

    // If the page has been locked, and we can't lock the page
382         if (unlikely(TryLockPage(page))) {
    // Do some IO operation
383             if (PageLaunder(page) && (gfp_mask & __GFP_FS)) {
384                 page_cache_get(page);
385                 spin_unlock(&pagemap_lru_lock);
386                 wait_on_page(page);
    // Release the reference to the page. If it reaches 0, the page will be freed.
387                 page_cache_release(page);
388                 spin_lock(&pagemap_lru_lock);
389             }
390             continue;
391         }
392 
    // The is_page_cache_freeable will return true if it is not mapped by any process and has no buffers.
    // So this handles a page is dirty, is not mapped by any process, has no buffers and is backed by a file or device mapping.
393         if (PageDirty(page) && 
                is_page_cache_freeable(page) && 
                page->mapping) {
394             /*
395              * It is not critical here to write it only if
396              * the page is unmapped beause any direct writer
397              * like O_DIRECT would set the PG_dirty bitflag
398              * on the phisical page after having successfully
399              * pinned it and after the I/O to the page is finished,
400              * so the direct writes to the page cannot get lost.
401              */
402             int (*writepage)(struct page *);
403 
404             writepage = page->mapping->a_ops->writepage;
405             if ((gfp_mask & __GFP_FS) && writepage) {
    // Clear the dirty bit and mark that the page is being laundered.
406                 ClearPageDirty(page);
407                 SetPageLaunder(page);
    // Take a reference to the page so it will not be freed unexpectely
408                 page_cache_get(page);
409                 spin_unlock(&pagemap_lru_lock);
410 
    // Call filesystem-specific writepage() function.
411                 writepage(page);
    // Now the page has locked and launder bit, it will become clean and will be reclaimed by the previous block of code.
412                 page_cache_release(page);
413 
414                 spin_lock(&pagemap_lru_lock);
415                 continue;
416             }
417         }
    
    // Page has buffers associated with it must be freed.
424         if (page->buffers) {
    // Release the LRU lock as we may sleep.
425             spin_unlock(&pagemap_lru_lock);
426 
427             /* avoid to free a locked page */
428             page_cache_get(page);
429 
    // try_to_release_page() will attempt to release the buffers associated with the page. Returns 1 if it succeeds.
430             if (try_to_release_page(page, gfp_mask)) {
431                 if (!page->mapping) {
    // This is a case where an anonymous page that was in the swap cache has now had it's buffers cleared and removed.
438                     spin_lock(&pagemap_lru_lock);
439                     UnlockPage(page);
440                     __lru_cache_del(page);
441 
442                     /* effectively free the page here */
443                     page_cache_release(page);
444 
445                     if (--nr_pages)
446                         continue;
447                     break;
448                 } else {
454                     page_cache_release(page);
455 
456                     spin_lock(&pagemap_lru_lock);
457                 }
458             } else {
459                 /* failed to drop the buffers so stop here */
460                 UnlockPage(page);
461                 page_cache_release(page);
462 
463                 spin_lock(&pagemap_lru_lock);
464                 continue;
465             }
466         }

468         spin_lock(&pagecache_lock);
469 
470         /*
471          * this is the non-racy check for busy page.
472          */
473         if (!page->mapping || !is_page_cache_freeable(page)) {
474             spin_unlock(&pagecache_lock);
475             UnlockPage(page);
476 page_mapped:
477             if (--max_mapped >= 0)
478                 continue;
479 
    // Too many mapped pages have been found in the page cache. The LRU lock is released and swap_out() is called to begin swapping out whole processes.
484             spin_unlock(&pagemap_lru_lock);
485             swap_out(priority, gfp_mask, classzone);
486             return nr_pages;
487         }

493         if (PageDirty(page)) {
494             spin_unlock(&pagecache_lock);
495             UnlockPage(page);
496             continue;
497         }
    
498 
499         /* point of no return */
500         if (likely(!PageSwapCache(page))) {
501             __remove_inode_page(page);
502             spin_unlock(&pagecache_lock);
503         } else {
504             swp_entry_t swap;
505             swap.val = page->index;
506             __delete_from_swap_cache(page);
507             spin_unlock(&pagecache_lock);
508             swap_free(swap);
509         }
510 
511         __lru_cache_del(page);
512         UnlockPage(page);
513 
514         /* effectively free the page here */
515         page_cache_release(page);
516 
517         if (--nr_pages)
518             continue;
519         break;
520     }
521     spin_unlock(&pagemap_lru_lock);
522 
523     return nr_pages;
524 }
```

### 6. Swapping Out Process Pages

#### 6.1 Function: swap_out (mm/vmscan.c)

This function linearly searches through every processes page tables trying to swap out `SWAP_CLUSTER_MAX` number of pages. The process it starts with is the `swap_mm` and the starting address is `mm->swap_address`.

```C
296 static int swap_out(unsigned int priority, unsigned int gfp_mask, 
            zone_t * classzone)
297 {
    // SWAP_CLUSTER_MAX is defined as 32
298     int counter, nr_pages = SWAP_CLUSTER_MAX;
299     struct mm_struct *mm;
300 
301     counter = mmlist_nr;
302     do {
303         if (unlikely(current->need_resched)) {
304             __set_current_state(TASK_RUNNING);
305             schedule();
306         }
307 
308         spin_lock(&mmlist_lock);
309         mm = swap_mm;
    // Move to the next process if the swap_address has reached the TASK_SIZE or if the mm is the init_mm.
310         while (mm->swap_address == TASK_SIZE || mm == &init_mm) {
    // Reset it's swap_address to 0.
311             mm->swap_address = 0;
312             mm = list_entry(mm->mmlist.next, 
                        struct mm_struct, mmlist);
313             if (mm == swap_mm)
314                 goto empty;
315             swap_mm = mm;
316         }
317 
318         /* Make sure the mm doesn't disappear 
             when we drop the lock.. */
319         atomic_inc(&mm->mm_users);
320         spin_unlock(&mmlist_lock);
321 
    // Begin scanning the mm with swap_out_mm().
322         nr_pages = swap_out_mm(mm, nr_pages, &counter, classzone);
323 
324         mmput(mm);
325 
326         if (!nr_pages)
327             return 1;
328     } while (--counter >= 0);
329 
330     return 0;
331 
332 empty:
333     spin_unlock(&mmlist_lock);
334     return 0;
335 }
```

#### 6.2 Function: swap_out_mm (mm/vmscan.c)

Walk through each VMA and call swap_out_vma() for each one.

```C
256 static inline int swap_out_mm(struct mm_struct * mm, int count, 
                  int * mmcounter, zone_t * classzone)
257 {
258     unsigned long address;
259     struct vm_area_struct* vma;
260 
265     spin_lock(&mm->page_table_lock);
266     address = mm->swap_address;
267     if (address == TASK_SIZE || swap_mm != mm) {
268         /* We raced: don't count this mm but try again */
269         ++*mmcounter;
270         goto out_unlock;
271     }
272     vma = find_vma(mm, address);
273     if (vma) {
274         if (address < vma->vm_start)
275             address = vma->vm_start;
276 
277         for (;;) {
278             count = swap_out_vma(mm, vma, address, 
                         count, classzone);
279             vma = vma->vm_next;
280             if (!vma)
281                 break;
282             if (!count)
283                 goto out_unlock;
284             address = vma->vm_start;
285         }
286     }
287     /* Indicate that we reached the end of address space */
288     mm->swap_address = TASK_SIZE;
289 
290 out_unlock:
291     spin_unlock(&mm->page_table_lock);
292     return count;
293 }
```

#### 6.3 Function: swap_out_vma (mm/vmscan.c)

Walk through this VMA and for each PGD in it, call swap_out_pgd().

```C
227 static inline int swap_out_vma(struct mm_struct * mm, 
                   struct vm_area_struct * vma, 
                   unsigned long address, int count, 
                   zone_t * classzone)
228 {
229     pgd_t *pgdir;
230     unsigned long end;
231 
232     /* Don't swap out areas which are reserved */
233     if (vma->vm_flags & VM_RESERVED)
234         return count;
235 
236     pgdir = pgd_offset(mm, address);
237 
238     end = vma->vm_end;
239     BUG_ON(address >= end);
240     do {
241         count = swap_out_pgd(mm, vma, pgdir, 
                     address, end, count, classzone);
242         if (!count)
243             break;
244         address = (address + PGDIR_SIZE) & PGDIR_MASK;
245         pgdir++;
246     } while (address && (address < end));
247     return count;
248 }
```

