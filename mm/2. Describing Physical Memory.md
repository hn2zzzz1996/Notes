# 2. Describing Physical Memory

Be attention, all of the code of function in the linux version 2.4.22, it must be obsolete. But for study reason, I record these functions to learn how these functions are designed in the early age.

## Nodes

Each node in memory is described by a `pg_data_t`.

```C
129 typedef struct pglist_data {
    // The zones for this node, ZONE_NORMAL, ZONE_DMA;
130     zone_t node_zones[MAX_NR_ZONES];
    // The order of zones that allocations are preferred from.
131     zonelist_t node_zonelists[GFP_ZONEMASK+1];
    // Number of zones in this node.
132     int nr_zones;
133     struct page *node_mem_map;
    // Ignored by x86.
134     unsigned long *valid_addr_bitmap;
135     struct bootmem_data *bdata;
    // The starting physical address of the node.
136     unsigned long node_start_paddr;
137     unsigned long node_start_mapnr;
    // The total number of pages in this zone.
138     unsigned long node_size;
139     int node_id;
140     struct pglist_data *node_next;
141 } pg_data_t; 
```

All nodes in the system are maintained on a list called `pgdat_list`. The nodes are placed on this list as they are initalized by the `init_bootmem_core()` function.

## Zones

Zones are described by a struct `zone_struct` and is usually referred to by it's typedef `zone_t`. It keeps track of information like page usage statistics, free area information and locks. It declared as follows:

```C
37 typedef struct zone_struct {
    // Spinlock to protect the zone from concurrent accesses.
41     spinlock_t        lock;
    // Toxtal number of free pages in the zone.
42     unsigned long     free_pages;
    // This is the zone watermarks
43     unsigned long     pages_min, pages_low, pages_high;
    // This flag that tells the pageout kswapd to balance the zone. A zone is said to need balance when the number of available pages reaches one of the zone watermarks.
44     int               need_balance;
45 
    // The free list used by buddy allocator.
49     free_area_t       free_area[MAX_ORDER];
50 
    // A hash table of wait queues of processes waiting on a page to be freed. This is of importance to wait_on_page() and unlock_page().
76     wait_queue_head_t * wait_table;
    // Number of queues in the hashtable which is a power of 2.
77     unsigned long     wait_table_size;
78     unsigned long     wait_table_shift;
79 
    // Points to parent node.
83     struct pglist_data *zone_pgdat;
84     struct page        *zone_mem_map;
85     unsigned long      zone_start_paddr;
86     unsigned long      zone_start_mapnr;
87 
    // The string name of the zone, "DMA", "Normal".
91     char               *name;
    // The size of the zone is pages.
92     unsigned long      size;
93 } zone_t;
```

Here is a question, how to identify a page belongs to which zone?

In the simple way, it must use a field to record the zone, like `page->zone`. But to save the space for the page. The linux kernel stored the zone information into the `page->flags`. And it use `page_zone` to get it.

```C
// It uses the highest several bits to store the zone it belongs to. And the zone information is stores by the zone_table.
static inline zone_t *page_zone(struct page *page)
{
	return zone_table[page->flags >> ZONE_SHIFT];
}
```

### Zone Watermarks

When available memory in the system is low, the pageout daemon `kswapd` is woken up to start freeing pages. If the pressure is high, the process will free up memory synchronously, sometimes referred to as the `direct-reclaim` path.

The `pages_min` always calculated by `ZoneSizeInPages/128`.

When the current number of free pages reaches any one of the watermarks, some action will be triggered:

* **pages_low**: When pages_low number of free pages is reached, kswapd is woken up by the buddy allocator to start freeing pages.
* **pages_min**: When pages_min reached, the allocator will do the kswapd work in a synchronous way, also called as direct-reclaim path.
* **pages_high**: Once kswapd has been woken to start freeing pages it will not consider the zone to be "balanced" when pages_high pages are free. Once the watermark has been reached, kswapd will go back to sleep.

## Pages

Every physical page frame in the system has an associated `struct page` which is used to keep track of its status.

```C
152 typedef struct page {
    // Pages may belong to many lists. For example, if page is file or devices memory mapped, it will linked to one of tree linked list kept by the address_space. These are clean_pages, dirty_pages and locked_pages.
    // In a slab allocator, this field is used to store pointers to the slab and cache the page belongs to.
    // Also used to link free pages together.
153     struct list_head list;
    // When file or devices are memory mapped, their inode has an associated address_space and this filed is point to this address_space.
    // If a page is anonymous mapped, when mapping is set, it will point to the swapper_space which manages the swap address space.
154     struct address_space *mapping;
    // If a page is part of file mapping, the index is the offset within the file.
    // If the page is part of the swap cache this will be the offset within the address_space for the swap address space(swapper_space).
    // If the page is free. It will be it's order.
155     unsigned long index;
    // Pages that are part of a file mapping are hashed on the inode and offset. This field links pages together that share the same hash bucket.
156     struct page *next_hash;
    // Reference count. If it drops to 0, it may be freed.
158     atomic_t count;
    // Flags which describe the status of the page.
159     unsigned long flags;
    // For the page replacement policy, pages that may be swapped out will exist on either the active_list or the inactive_list.
161     struct list_head lru;
	// The pair to the next_hash. Which makes the hash can work as a doubly linked list.
163     struct page **pprev_hash;
    // If a page has buffers for a block device associated with it, this field is used to keep track of the buffer_head.
    // An anonymous page mapped by a process may also have an associated buffer_head if it is backed by a swap file. This is necessary as the page has to be synced with backing storage in block sized chunks in block sized chunks defined by the unerlying filesystem.
164     struct buffer_head * buffers;
175
176 #if defined(CONFIG_HIGHMEM) || defined(WANT_PAGE_VIRTUAL)
177     void *virtual;
179 #endif /* CONFIG_HIGMEM || WANT_PAGE_VIRTUAL */
180 } mem_map_t;
```

There are some flags defined to describe the status of a page:

| Bit name      | Description                                                  |
| ------------- | ------------------------------------------------------------ |
| PG_active     | This bit is set if a page is on the active_list LRU and cleared when it is removed. It marks a page as being hot. |
| PG_dirty      | This indicated if a page needs to be flushed to disk. When a page is written to that is backed by disk, it is not flushed immediately, this bit needed to ensure a dirty page is not freed before it is written out. |
| PG_error      | If an error occurs during disk I/O, this bit is set.         |
| PG_launder    | This bit is important only to the page replacement policy. When the VM wants to swap out a page, it will set this bit and call the writepage() function. When scanning, if it encounters a page with this bit and PG_locked set, it will wait for the I/O to complete. |
| PG_locked     | This bit is set when the page must be locked in memory for disk I/O. When I/O starts, this bit is set and released when it completes. |
| PG_lru        | If a page is on either the active_list or the inactive_list, this bit will be set. |
| PG_referenced | If a page is mapped and it is referenced through the mapping, index hash table, this bit is set. It is used during page replacement for moving the page around the LRU lists. |
| PG_slab       | This will flag a page as being used by the slab allocator.   |
| PG_uptodate   | When a page is read from disk without error, this bit will be set. |

## Functions about zones

### Function: zone_sizes_init()

This is the top-level function that is used to initialize each of the zones. And will get the `zones_size` that used later.

### Function: free_area_init()

This is the architecture-independent function for setting up a UMA architecture. It simply calls the core function passing the static `contig_page_data` as the node.

```C
void __init free_area_init(unsigned long *zones_size)
{
    free_area_init_core(0, &contig_page_data, &mem_map,
                       zones_size, 0, 0, 0);
}
```

### Function: free_area_init_node()

```C
// @pmap: is a pointer to the portion of the mem_map for this node to use, whici is frequently passed as NULL and allocated later.
void free_area_init_node(int nid, pg_data_t *pgdat,
							struct page *pmap, unsigned long zones_size,
                               unsigned long zone_start_paddr,
                               unsigned long *zholes_size)
{
    // global memmap is no need for NUMA, so pass a discard.
    free_area_init_core(nid, pgdat, &discard, zones_size,
                       zone_start_paddr, zholes_size, pmap);
}
```

### Function: free_area_init_core()

This function is responsible for initializing all zones and allocating their local `lmem_map` within a node. In UMA architectures, this function is called in a way that will initialize the global `mem_map` array. In NUMA architectures, the array is treated as a virtual array is sparsely populated.

```C
/*
 * @nid: Node Identifier(nid) for the node.
 * @gmap: global memmap.
 * @lmem_map: local mem_map for the node which is used by NUMA architecture.
 */
void __init free_area_init_core(int nid, pg_data_t *pgdat,
                               struct page **gmap, unsigned long zones_size,
                               unsigned long zone_start_paddr,
                               unsigned long *zholes_size,
                               struct page *lmem_map);
```

## Page Operations

### Locking Pages

#### Function: lock_page() (mm/filemap.c)

This function tries to lock a page. If the page cannot be locked, it will cause the process to sleep until the page is available.

```C
void lock_page(struct page *page)
{
	if (TryLockPage(page))
		__lock_page(page);
}
```

`TryLockPage()` is just a wrapper around `test_and_set_bit()` for the `PG_locked` bit in `page-flags`.

#### Function: __lock_page() (mm/filemap.c)

This is called after a `TryLockPage()` failed. It will locate the waitqueue for this page and sleep on it until the lock can be acquired.

```C
static void __lock_page(struct page *page)
{
	wait_queue_head_t *waitqueue = page_waitqueue(page);
	struct task_struct *tsk = current;
	DECLARE_WAITQUEUE(wait, tsk);

	add_wait_queue_exclusive(waitqueue, &wait);
	for (;;) {
		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
		if (PageLocked(page)) {
			sync_page(page);
			schedule();
		}
		if (!TryLockPage(page))
			break;
	}
	__set_task_state(tsk, TASK_RUNNING);
	remove_wait_queue(waitqueue, &wait);
}
```

#### Function: sync_page() (mm/filemap.c)

This calls the filesystem-specific `sync_page()` to synchronize the page with its backing storage.

```C
static inline int sync_page(struct page *page)
{
    /* page->mapping. When files or devices are memory mapped, their inode has an associated address_space. This field will point to this address space if the page belongs to the file. */
	struct address_space *mapping = page->mapping;

	if (mapping && mapping->a_ops && mapping->a_ops->sync_page)
		return mapping->a_ops->sync_page(page);
	return 0;
}
```

### Unlocking Pages

#### Function: unlock_page() (mm/filemap.c)

This function unlocks a page and wakes up any processes that may be waiting on it.

#### Function: wait_on_page() (include/linux/pagemap.h)

If the page is currently locked, this calls `__wait_on_page()` to sleep until it is unlocked.

```C
static inline void wait_on_page(struct page * page)
{
	if (PageLocked(page))
		___wait_on_page(page);
}
```

#### Function: __wait_on_page() (mm/filemap.c)

Sleep until page is unlocked.

```C
void ___wait_on_page(struct page *page)
{
	wait_queue_head_t *waitqueue = page_waitqueue(page);
	struct task_struct *tsk = current;
	DECLARE_WAITQUEUE(wait, tsk);

	add_wait_queue(waitqueue, &wait);
	do {
		set_task_state(tsk, TASK_UNINTERRUPTIBLE);
		if (!PageLocked(page))
			break;
		sync_page(page);
		schedule();
	} while (PageLocked(page));
	__set_task_state(tsk, TASK_RUNNING);
	remove_wait_queue(waitqueue, &wait);
}
```



