# 4. Process Virtual Address

## Managing the Address Space

Each address space consists a number of `page-aligned` regions of memory that are in use. They never overlap and represent a set of address which contain pages that are related to each other in terms of protection and purpose.

These regions are represented by `struct vm_area_struct`.

Here are system calls which related to memory regions:

| System Call | Description                                                  |
| ----------- | ------------------------------------------------------------ |
| fork()      | Creates a new process with a new address space. All the pages are marked COW and are shared between the two processes until a page fault occurs to make private copies. |
| clone()     | clone() allows a new process to be created that shares parts of its context with its parent and is how threading is implemented in Linux. clone() without the CLONE_VM set will create a new address space which is essentially the same as fork(). |
| mmap()      | mmap() creates a new region within the process linear address space. |
| mremap()    | Remaps or resizes a region of memory. If the virtual address space is not available for the mapping, the region may be moved unless the move is forbidden by the caller. |
| munmap()    | This destroys part or all of a region. If the region been unmapped is in the middle of an existing region, the existing region is split into two separate regions. |
| shmat()     | This attaches a shared memory segment to a process address space. |
| shmdt()     | Removes a shared memory segment from an address space.       |
| execve()    | This loads a new executable file replacing the current address space. |
| exit()      | Destroys an address space and all regions.                   |

### Process Address Space Descriptor

A process address space is described by the `mm_struct` struct. Also, for threads, they both have pointers to the same `mm_struct`, so they can both access the memory in this process. And this the way to identity thread.

A unique `mm_struct` is not needed for `kernel threads` because they will never page fault or access the userspace portion. The only exception is page faulting within the vmalloc space.

Each processs has a pointer (mm_struct->pgd) to its own Page Global Directory (PGD) which is a physical page frame.

```C
206 struct mm_struct {
    // The head of a linked list of all VMA regions in the address space;
207     struct vm_area_struct * mmap;
    // The VMAs are arranged in a linked list and in a red-block tree for fast lookups. This is the root of the tree.
208     rb_root_t mm_rb;
    // The VMA found during the last call to find_vma() is stored in this field on the assumption that the area will be used again soon.
209     struct vm_area_struct * mmap_cache;
210     pgd_t * pgd;
    // mm_users is a reference count of processes accessing the userspace portion of this mm_struct, such as page tables and file mappings. Threads and the swap_out() will increament this count making sure a mm_struct is not destroyed early. When it drops to 0, exit_mmap() will delete all mappings and tear down that page tables before decrementing the mm_count.
211     atomic_t mm_users;
	// mm_count is a reference count of the "anonymous users" for the mm_struct, and mm_count is initilized at 1 for the "real" user. An anonymous user is one that does not necessarily care about the userspace portion and is just borrowing the mm_struct.
    // The refereces count is needed due to some user need the mm_struct to exitst even if the userspace mappings get destroyed and there is no point delaying the teardown of the page tables.
212     atomic_t mm_count;
    // Number of VMAs in use.
213     int map_count;
    // This is a long lived lock which protects the VMA list.
214     struct rw_semaphore mmap_sem;
    // This protects page tables, and the VMA modification.
215     spinlock_t page_table_lock;
216 
    // All mm_structs are linked together via this field.
217     struct list_head mmlist;
221 
222     unsigned long start_code, end_code, start_data, end_data;
223     unsigned long start_brk, brk, start_stack;
224     unsigned long arg_start, arg_end, env_start, env_end;
225     unsigned long rss, total_vm, locked_vm;
226     unsigned long def_flags;
227     unsigned long cpu_vm_mask;
    // Used by the pageout daemon to record the last address that was swapped from when swapping out entire processes.
228     unsigned long swap_address;
229 
230     unsigned dumpable:1;
231 
232     /* Architecture-specific MM context */
233     mm_context_t context;
234 };
```

There are a small number of functions for dealing with mm_structs.

| Function      | description                                                  |
| ------------- | ------------------------------------------------------------ |
| mm_init()     | Initialises a `mm_struct` by setting starting values for each field, allocating a PGD, initialising spinlocks etc. |
| allocate_mm() | Allocates a `mm_struct()` from the slab allocator            |
| mm_alloc()    | Allocates a `mm_struct` using `allocate_mm()` and calls `mm_init()` to initialise it |
| exit_mmap()   | Walks through a `mm_struct` and unmaps all VMAs associated with it |
| copy_mm()     | Makes an exact copy of the current tasks `mm_struct` for a new task. This is only used during fork |
| free_mm()     | Returns the `mm_struct` to the slab allocator                |

### Memory Regions

The full address space of a process is rarely used, only sparse regions are. Each region is represented by a `vm_area_struct` which never overlap and represent a set of addresses with the same protection and purpose.

If the region is backed by a file, the `struct file` is available through the `vm_file` field which has a pointer to the `struct inode`. The inode is used to get the `struct address_space` which has all the private information about the file including a set of pointers of filesystem functions which perform the filesystem specific operations such as reading and writing pages to disk.

```C
 44 struct vm_area_struct {
     // The mm_struct this VMA belongs to.
 45     struct mm_struct * vm_mm;
     // Starting and end address of this region.
 46     unsigned long vm_start;
 47     unsigned long vm_end;
 49 
 50     /* linked list of VM areas per task, sorted by address */
 51     struct vm_area_struct *vm_next;
 52 
     // The protection flags that are set for each PTE in this VMA.
 53     pgprot_t vm_page_prot;
     // Flags describing the protections and properties of the VMA.
 54     unsigned long vm_flags;
 55 
 56     rb_node_t vm_rb;
 57 
     // Shared VMA regions based on file mappings (such as shared libraries) linked together with this field.
 63     struct vm_area_struct *vm_next_share;
 64     struct vm_area_struct **vm_pprev_share;
 65 
 66     /* Function pointers to deal with this struct. */
 67     struct vm_operations_struct * vm_ops;
 68 
 69     /* Information about our backing store: */
     // This is the page aligned offset within a file that is memory mapped.
 70     unsigned long vm_pgoff;
     // The struct file pointer to the file being mapped;
 72     struct file * vm_file;
 73     unsigned long vm_raend;
     // Used by some device drivers to store private information.
 74     void * vm_private_data;
 75 };
```

| **Locking Flags** |                                                              |
| ----------------- | ------------------------------------------------------------ |
| `VM_LOCKED`       | If set, the pages will not be swapped out. Set by mlock()    |
| `VM_IO`           | Signals that the area is a mmaped region for IO to a device. It will also prevent the region being core dumped |
| `VM_RESERVED`     | Do not swap out this region, used by device drivers          |

#### Memory Region Operations

```C
133 struct vm_operations_struct {
134     void (*open)(struct vm_area_struct * area);
135     void (*close)(struct vm_area_struct * area);
136     struct page * (*nopage)(struct vm_area_struct * area, 
                                unsigned long address, 
                                int unused);
137 };
```

The `open()` and `close()` functions will be called every time a region is created or deleted. These functions are only used by a small number of devices.

The main operation of interest is the `nopage()` callback. This callback is used during a page-fault by `do_no_page()`. The callback is responsible for locating the page in the page cache or allocating a page and populating it with the required data before returning it.

Most files that are mapped will use a generic `vm_operations_struct()` called `generic_file_vm_ops`. It registers only a `nopage()` function called `filemap_nopage()`. This `nopage()` function will either locating the page in the page cache or read the information from disk. The struct is declared as follows in `mm/filemap.c`:

```C
2243 static struct vm_operations_struct generic_file_vm_ops = {
2244     nopage:         filemap_nopage,
2245 };
```

#### File/Device backed memory regions

If a region is backed by a file, the `vm_file` leads to an associated `address_space`. The struct contains information of relevance to the filesystem such as the number of dirty pages which must be flushed to disk.

```C
406 struct address_space {
    // List of clean pages that need no synchronisation with backing storage;
407     struct list_head        clean_pages;
    // List of dirty pages that need synchronisation with backing storage;
408     struct list_head        dirty_pages;    
    // List of pages that are locked in memory.
409     struct list_head        locked_pages;  
    // Number of resident pages in use by the address space.
410     unsigned long           nrpages;        
    // A struct of function for manipulating the filesystem.
411     struct address_space_operations *a_ops; 
    // The host inode the file belongs to.
412     struct inode            *host;    
    // A list of private mappings using this address_space.
413     struct vm_area_struct   *i_mmap;        
    // A list of VMAs which share mappings in this address_space.
414     struct vm_area_struct   *i_mmap_shared; 
    // A spinlock to protect this structure.
415     spinlock_t              i_shared_lock;  
    // The mask to use when calling __alloc_pages() for new pages.
416     int                     gfp_mask;       
417 };
```

Periodically the memory manager will need to flush information to disk. The memory manager does not know and does not care how information is written to disk, so the `a_ops` struct is used to call the relevant functions.

```C
385 struct address_space_operations {
386     int (*writepage)(struct page *);
387     int (*readpage)(struct file *, struct page *);
388     int (*sync_page)(struct page *);
389     /*
390      * ext3 requires that a successful prepare_write() call be
391      * followed by a commit_write() call - they must be balanced
392      */
393     int (*prepare_write)(struct file *, struct page *, 
                             unsigned, unsigned);
394     int (*commit_write)(struct file *, struct page *, 
                             unsigned, unsigned);
395     /* Unfortunately this kludge is needed for FIBMAP. 
         * Don't use it */
396     int (*bmap)(struct address_space *, long);
397     int (*flushpage) (struct page *, unsigned long);
398     int (*releasepage) (struct page *, int);
399 #define KERNEL_HAS_O_DIRECT
400     int (*direct_IO)(int, struct inode *, struct kiobuf *, 
                         unsigned long, int);
401 #define KERNEL_HAS_DIRECT_FILEIO
402     int (*direct_fileIO)(int, struct file *, struct kiobuf *, 
                             unsigned long, int);
403     void (*removepage)(struct page *);
404 };
```



## Functions

### 1. Process Memory Descriptors

This section covers the functions used to allocate, initialize, copy and destroy memory descriptors.

#### 1.1 Initializing a Descriptor

The initial `mm_struct` in the system is statically initialized at compile time using the macro INIT_MM().

```C
#define INIT_MM(name) \
{			 				\
	mm_rb:		RB_ROOT,			\
	pgd:		swapper_pg_dir, 		\
	mm_users:	ATOMIC_INIT(2), 		\
	mm_count:	ATOMIC_INIT(1), 		\
	mmap_sem:	__RWSEM_INITIALIZER(name.mmap_sem), \
	page_table_lock: SPIN_LOCK_UNLOCKED, 		\
	mmlist:		LIST_HEAD_INIT(name.mmlist),	\
}
```

After, every new `mm_struct` is created by copying it's parent's `mm_struct` using `copy_mm()`.

#### 1.2 Copying a descriptor

##### 1.2.1 Function: copy_mm() (kernel/fork.c)

This function makes a copy of the `mm_struct` for the given task. Only called from `do_fork()`.

```C
static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
{
	struct mm_struct * mm, *oldmm;
	int retval;

    // Initialize the task_struct fields related to memory management.
	tsk->min_flt = tsk->maj_flt = 0;
	tsk->cmin_flt = tsk->cmaj_flt = 0;
	tsk->nswap = tsk->cnswap = 0;

	tsk->mm = NULL;
	tsk->active_mm = NULL;

	/*
	 * Are we cloning a kernel thread?
	 *
	 * We need to steal a active VM for that..
	 */
	oldmm = current->mm;
    // A kernel thread has no mm, so it can return immediately.
	if (!oldmm)
		return 0;

    // If CLONE_VM is set, the child process is to share the mm with the parent.
	if (clone_flags & CLONE_VM) {
		atomic_inc(&oldmm->mm_users);
		mm = oldmm;
		goto good_mm;
	}

	retval = -ENOMEM;
	mm = allocate_mm();
	if (!mm)
		goto fail_nomem;

	/* Copy the current MM stuff.. */
	memcpy(mm, oldmm, sizeof(*mm));
	if (!mm_init(mm))
		goto fail_nomem;

	if (init_new_context(tsk,mm))
		goto free_pt;

	down_write(&oldmm->mmap_sem);
    // Copying all the VMA's regions in use by the parent process.
	retval = dup_mmap(mm);
	up_write(&oldmm->mmap_sem);

	if (retval)
		goto free_pt;

	/*
	 * child gets a private LDT (if there was an LDT in the parent)
	 */
	copy_segments(tsk, mm);

good_mm:
	tsk->mm = mm;
	tsk->active_mm = mm;
	return 0;

free_pt:
	mmput(mm);
fail_nomem:
	return retval;
}
```

##### 1.2.2 Function: mm_init() (kernel/fork.c)

The function initializes process-specific mm fields.

```C
static struct mm_struct * mm_init(struct mm_struct * mm)
{
	atomic_set(&mm->mm_users, 1);
	atomic_set(&mm->mm_count, 1);
	init_rwsem(&mm->mmap_sem);
	mm->page_table_lock = SPIN_LOCK_UNLOCKED;
	mm->pgd = pgd_alloc(mm);
    // By default, pages used by the process are not locked in memory.
	mm->def_flags = 0;
	if (mm->pgd)
		return mm;
	free_mm(mm);
	return NULL;
}
```

#### 1.3 Allocating a Descriptor

##### 1.3.1 Function: allocate_mm() (kernel/fork.c)

```C
#define allocate_mm() (kmem_cache_alloc(mm_cachep, SLAB_KERNEL))
```

##### 1.3.2 Function: mm_alloc() (kernel/fork.c)

```C
struct mm_struct * mm_alloc(void)
{
	struct mm_struct * mm;

	mm = allocate_mm();
	if (mm) {
		memset(mm, 0, sizeof(*mm));
		return mm_init(mm);
	}
	return NULL;
}
```

#### 1.4 Destroying a Descriptor

There are two reference count in the mm_struct.

* **mm_users**: Decremented with `mmput()`. If the `mm_users` reaches zero, all the mapped regions are deleted with `exit_mmap()`, and the pagetables are destroyed because there are no longer any users of the userspace portions.
* **mm_count**: The `mm_count` is decremented with `mmdrop()` because all the users of the pagetables and VMAs are counted as one mm_struct user. When mm_count reaches zero, the mm_struct will be destroyed.

##### 1.4.1 Funtion: mmput() (kernel/fork.c)

```C
void mmput(struct mm_struct *mm)
{
    // Atomically decrements the mm_users field while holding the mmlist_lock lock. It returns with the lock held if the count reaches zero.
	if (atomic_dec_and_lock(&mm->mm_users, &mmlist_lock)) {
		extern struct mm_struct *swap_mm;
		if (swap_mm == mm)
			swap_mm = list_entry(mm->mmlist.next, struct mm_struct, mmlist);
		list_del(&mm->mmlist);
		mmlist_nr--;
		spin_unlock(&mmlist_lock);
        // Removes all associated mappings.
		exit_mmap(mm);
        // Deletes the mm struct.
		mmdrop(mm);
	}
}
```

##### 1.4.2 Function: mmdrop() (include/linux/sched.h)

```C
static inline void mmdrop(struct mm_struct * mm)
{
	if (atomic_dec_and_test(&mm->mm_count))
		__mmdrop(mm);
}
```

Atomically decrement the reference count. If the count reaches zero, call `__mmdrop()`.

The mm_count could be higher if the mm was used by lazy tlb switching tasks.

##### 1.4.3 Function: __mmdrop() (kernel/fork.c)

```C
/*
 * Called when the last reference to the mm
 * is dropped: either by a lazy thread or by
 * mmput. Free the page directory and the mm.
 */
inline void __mmdrop(struct mm_struct *mm)
{
	BUG_ON(mm == &init_mm);
    // Deletes the PGD entry
	pgd_free(mm->pgd);
	destroy_context(mm);
    // Free it to slab allocator.
	free_mm(mm);
}
```

### 2. Creating Memory Regions

This large section deals with the creation, deletion and manipulation of memory regions.

#### 2.1 Creating Memory Regions

##### 2.1.1 Function: do_mmap() (include/linux/mm.h)

This is a very simple wrapper function around `do_mmap_pgoff()`, which performs most of the work.

```C
static inline unsigned long do_mmap(struct file *file, unsigned long addr,
	unsigned long len, unsigned long prot,
	unsigned long flag, unsigned long offset)
{
	unsigned long ret = -EINVAL;
	if ((offset + PAGE_ALIGN(len)) < offset)
		goto out;
    // offset align with page size.
	if (!(offset & ~PAGE_MASK))
		ret = do_mmap_pgoff(file, addr, len, prot, flag, offset >> PAGE_SHIFT);
out:
	return ret;
}
```

##### 2.1.2 Function: do_mmap_pgoff() (mm/mmap.c)

This function is very large, and it do the follow things:

* Sanity check the parameters.
* Find a free linear address space large enough for the memory mapping. If a filesystem or device-specific `get_unmapped_area()` function is provided, it will be used. Otherwise, `arch_get_unmapped_area()` is called.
* Calculate the VM flags and check them against the file access permissions.
* If an old area exists where mapping is to take place, fix it so it is suitable for the new mapping. 
* Allocate a `vm_area_struct` from the slab allocator and fill in its entries.
* Link in the new VMA.
* Call the filesystem or device-specific mmap() function.
* Update statistics and exit.

```C
/**
 * @file: The struct file to mmap if this is a file-backed mapping
 * @addr: The requested address to map
 * @len: The length in bytes to map
 * @prot: The permissions on the area
 * @flags: The flags for the mapping
 * @pgoff: The offset within the file to begin the mmap at
 */
unsigned long do_mmap_pgoff(struct file * file, unsigned long addr, unsigned long len,
	unsigned long prot, unsigned long flags, unsigned long pgoff)
{
    struct mm_struct * mm = current->mm;
	struct vm_area_struct * vma, * prev;
	unsigned int vm_flags;
	int correct_wcount = 0;
	int error;
	rb_node_t ** rb_link, * rb_parent;
    
    /* ----------------------Sanity Check part start-----------------------*/
    
    // If it's a file or device is mapped. For most filesystems, this will call generic_file_mmap().
	if (file && (!file->f_op || !file->f_op->mmap))
		return -ENODEV;

	if (!len)
		return addr;

	len = PAGE_ALIGN(len);

    // TASK_SIZE is equal to PAGE_OFFSET
	if (len > TASK_SIZE || len == 0)
		return -EINVAL;

	/* offset overflow? */
	if ((pgoff + (len >> PAGE_SHIFT)) < pgoff)
		return -EINVAL;

	/* Too many mappings? */
    // By default, max_map_count is DEFAULT_MAX_MAP_COUNT or 65536.
	if (mm->map_count > max_map_count)
		return -ENOMEM;
    
    /* ----------------------Sanity Check part end-----------------------*/
    
    /* ----------------------Find free linear address-----------------------*/
	/* Obtain the address to map to. we verify (or select) it and ensure
	 * that it represents a valid section of the address space.
	 */
	addr = get_unmapped_area(file, addr, len, pgoff, flags);
	if (addr & ~PAGE_MASK)
		return addr;
    
    /* ----------------------Find free linear address-----------------------*/
    
    /* Do simple checking here so the lower-level routines won't have
	 * to. we assume access permissions have been handled by the open
	 * of the memory object, so we don't do any here.
	 */
    // calc_vm_flags() translates the prot and flags from userspace and translates them to their VM_ equivalents.
	vm_flags = calc_vm_flags(prot,flags) | mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;

	/* mlock MCL_FUTURE? */
    // Check if it has been requested that all future mappings be locked in memory. If yes, it makes sure the process isn't locking more memory that it is allowed to.
	if (vm_flags & VM_LOCKED) {
		unsigned long locked = mm->locked_vm << PAGE_SHIFT;
		locked += len;
		if (locked > current->rlim[RLIMIT_MEMLOCK].rlim_cur)
			return -EAGAIN;
	}
    
    if (file) {
        // If a file is memory mapped, checks the files access permissions.
		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			if ((prot & PROT_WRITE) && !(file->f_mode & FMODE_WRITE))
				return -EACCES;

			/* Make sure we don't allow writing to an append-only file.. */
			if (IS_APPEND(file->f_dentry->d_inode) && (file->f_mode & FMODE_WRITE))
				return -EACCES;

			/* make sure there are no mandatory locks on the file. */
			if (locks_verify_locked(file->f_dentry->d_inode))
				return -EAGAIN;

			vm_flags |= VM_SHARED | VM_MAYSHARE;
			if (!(file->f_mode & FMODE_WRITE))
				vm_flags &= ~(VM_MAYWRITE | VM_SHARED);

			/* fall through */
		case MAP_PRIVATE:
            // Make sure the file can be read before mmapping it.
			if (!(file->f_mode & FMODE_READ))
				return -EACCES;
			break;

		default:
			return -EINVAL;
		}
	} else {
		vm_flags |= VM_SHARED | VM_MAYSHARE;
		switch (flags & MAP_TYPE) {
		default:
			return -EINVAL;
		case MAP_PRIVATE:
			vm_flags &= ~(VM_SHARED | VM_MAYSHARE);
			/* fall through */
		case MAP_SHARED:
			break;
		}
	}
    
    /* Clear old maps */
munmap_back:
    // find_vma_prepare() step through the RB tree for the VMA corresponding to a given address.
	vma = find_vma_prepare(mm, addr, &prev, &rb_link, &rb_parent);
	if (vma && vma->vm_start < addr + len) {
        // If a VMA was found and it is part of the new mapping, this removes the old mapping because the new one will cover both.
		if (do_munmap(mm, addr, len))
			return -ENOMEM;
		goto munmap_back;
	}
    
    /* Check against address space limit. */
	if ((mm->total_vm << PAGE_SHIFT) + len
	    > current->rlim[RLIMIT_AS].rlim_cur)
		return -ENOMEM;

	/* Private writable mapping? Check memory availability.. */
	if ((vm_flags & (VM_SHARED | VM_WRITE)) == VM_WRITE &&
	    !(flags & MAP_NORESERVE)				 &&
	    !vm_enough_memory(len >> PAGE_SHIFT))
		return -ENOMEM;

	/* Can we just expand an old anonymous mapping? */
	if (!file && !(vm_flags & VM_SHARED) && rb_parent)
		if (vma_merge(mm, prev, rb_parent, addr, addr + len, vm_flags))
			goto out;
    
    /* Determine the object being mapped and call the appropriate
	 * specific mapper. the address has already been validated, but
	 * not unmapped, but the maps are removed from the list.
	 */
	vma = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);
	if (!vma)
		return -ENOMEM;

	vma->vm_mm = mm;
	vma->vm_start = addr;
	vma->vm_end = addr + len;
	vma->vm_flags = vm_flags;
	vma->vm_page_prot = protection_map[vm_flags & 0x0f];
	vma->vm_ops = NULL;
	vma->vm_pgoff = pgoff;
	vma->vm_file = NULL;
	vma->vm_private_data = NULL;
	vma->vm_raend = 0;

    if (file) {
		error = -EINVAL;
        // Invalid flags for a file mapping
		if (vm_flags & (VM_GROWSDOWN|VM_GROWSUP))
			goto free_vma;
		if (vm_flags & VM_DENYWRITE) {
			error = deny_write_access(file);
			if (error)
				goto free_vma;
			correct_wcount = 1;
		}
        // vma point to the file
		vma->vm_file = file;
		get_file(file);
        // In many case call generic_file_mmap().
		error = file->f_op->mmap(file, vma);
		if (error)
			goto unmap_and_free_vma;
	} else if (flags & MAP_SHARED) {
        // Setup anonymous shared mapping.
		error = shmem_zero_setup(vma);
		if (error)
			goto free_vma;
	}
    
	/* Can addr have changed??
	 *
	 * Answer: Yes, several device drivers can do it in their
	 *         f_op->mmap method. -DaveM
	 */
	if (addr != vma->vm_start) {
		/*
		 * It is a bit too late to pretend changing the virtual
		 * area of the mapping, we just corrupted userspace
		 * in the do_munmap, so FIXME (not in 2.4 to avoid breaking
		 * the driver API).
		 */
		struct vm_area_struct * stale_vma;
		/* Since addr changed, we rely on the mmap op to prevent 
		 * collisions with existing vmas and just use find_vma_prepare 
		 * to update the tree pointers.
		 */
		addr = vma->vm_start;
		stale_vma = find_vma_prepare(mm, addr, &prev,
						&rb_link, &rb_parent);
		/*
		 * Make sure the lowlevel driver did its job right.
		 */
		if (unlikely(stale_vma && stale_vma->vm_start < vma->vm_end)) {
			printk(KERN_ERR "buggy mmap operation: [<%p>]\n",
				file ? file->f_op->mmap : NULL);
			BUG();
		}
	}

	vma_link(mm, vma, prev, rb_link, rb_parent);
	if (correct_wcount)
		atomic_inc(&file->f_dentry->d_inode->i_writecount);

out:	
	mm->total_vm += len >> PAGE_SHIFT;
	if (vm_flags & VM_LOCKED) {
		mm->locked_vm += len >> PAGE_SHIFT;
		make_pages_present(addr, addr + len);
	}
	return addr;

unmap_and_free_vma:
	if (correct_wcount)
		atomic_inc(&file->f_dentry->d_inode->i_writecount);
	vma->vm_file = NULL;
	fput(file);

	/* Undo any partial mapping done by a device driver. */
	zap_page_range(mm, vma->vm_start, vma->vm_end - vma->vm_start);
free_vma:
	kmem_cache_free(vm_area_cachep, vma);
	return error;
}
```

#### 2.2 Inserting a Memory Region

##### 2.2.1 Function: __insert_vm_struct() (mm/mmap.c)

There are a similar function `insert_vm_struct()`, but it's rarely used due to it not increase the `map_count`(Number of VMAs in use).

The `__insert_vm_struct`()  is used for inserting a new vma into an address space.

```C
void __insert_vm_struct(struct mm_struct * mm, struct vm_area_struct * vma)
{
	struct vm_area_struct * __vma, * prev;
	rb_node_t ** rb_link, * rb_parent;

    // It will be inserted between prev and __vma
	__vma = find_vma_prepare(mm, vma->vm_start, &prev, &rb_link, &rb_parent);
	if (__vma && __vma->vm_start < vma->vm_end)
		BUG();
	__vma_link(mm, vma, prev, rb_link, rb_parent);
	mm->map_count++;
	validate_mm(mm);
}
```

##### 2.2.2 Function: find_vma_prepare() (mm/mmap.c)

This is responsible for finding the correct places to insert a VMA at the supplied address. The forward VMA to link to returned with return. `pprev` is the previous node, which is required because the list is a singly linked list.

`rb_parent` is the parent node, `rb_link` is the leaf node.

```C
static struct vm_area_struct * 
    find_vma_prepare(struct mm_struct * mm, unsigned long addr,
						struct vm_area_struct ** pprev,
						rb_node_t *** rb_link, rb_node_t ** rb_parent)
{
	struct vm_area_struct * vma;
	rb_node_t ** __rb_link, * __rb_parent, * rb_prev;

	__rb_link = &mm->mm_rb.rb_node;
	rb_prev = __rb_parent = NULL;
	vma = NULL;

	while (*__rb_link) {
		struct vm_area_struct *vma_tmp;

		__rb_parent = *__rb_link;
		vma_tmp = rb_entry(__rb_parent, struct vm_area_struct, vm_rb);

		if (vma_tmp->vm_end > addr) {
			vma = vma_tmp;
			if (vma_tmp->vm_start <= addr)
				return vma;
            // vma_tmp->vm_start > addr && addr < vma_tmp->vm_end
			__rb_link = &__rb_parent->rb_left;
		} else {
            // vma_tmp->vm_end <= addr
            // rb_prev is smaller than __rb_link.
			rb_prev = __rb_parent;
			__rb_link = &__rb_parent->rb_right;
		}
	}

	*pprev = NULL;
	if (rb_prev)
        // The pprev is which it's vm_end <= addr, and is the most nearlist one.
		*pprev = rb_entry(rb_prev, struct vm_area_struct, vm_rb);
    // rb_link is the current node.
	*rb_link = __rb_link;
    // rb_parent is the parent node.
    // Sample
    //          8
    //        /   \
    //       4     6
    // If rb_link is 6, 8 is the rb_parent.
	*rb_parent = __rb_parent;
	return vma;
}
```

##### 2.2.3 Function: vma_link() (mm/mmap.c)

This is the top-level function for linking a VMA into the proper lists. It is responsible for acquiring the necessary locks to make a safe insertion.

```C
static inline void vma_link(struct mm_struct * mm, 
                            struct vm_area_struct * vma, 
                            struct vm_area_struct * prev,
			    rb_node_t ** rb_link, rb_node_t * rb_parent)
{
    // Acquires the spinlock that protects the address_space representing the file that is memory mapped.
	lock_vma_mappings(vma);
    // Acquires the pagetable lock, which protects the whole mm_struct.
	spin_lock(&mm->page_table_lock);
	__vma_link(mm, vma, prev, rb_link, rb_parent);
	spin_unlock(&mm->page_table_lock);
	unlock_vma_mappings(vma);

	mm->map_count++;
	validate_mm(mm);
}
```

##### 2.2.4 Function: __vma_link() (mm/mmap.c)

```C
static void __vma_link(struct mm_struct * mm, struct vm_area_struct * vma,  struct vm_area_struct * prev,
		       rb_node_t ** rb_link, rb_node_t * rb_parent)
{
    // Links the VMA into the linear-linked lists of VMAs in this mm through the vm_next field.
	__vma_link_list(mm, vma, prev, rb_parent);
	__vma_link_rb(mm, vma, rb_link, rb_parent);
	__vma_link_file(vma);
}
```

##### 2.2.5 Function: __vma_link_list() (mm/mmap.c)

```C
static inline void __vma_link_list(struct mm_struct * mm, 
                                   struct vm_area_struct * vma, 
                                   struct vm_area_struct * prev,
				   rb_node_t * rb_parent)
{
	if (prev) {
        // It has prev, means is will be inserted to the middle of the list
		vma->vm_next = prev->vm_next;
		prev->vm_next = vma;
	} else {
        // It does'not has prev, means it is the smallest vma, there are two situations.
		mm->mmap = vma;
        // One is has a parent, so it will be the head of the linked list.
		if (rb_parent)
			vma->vm_next = rb_entry(rb_parent, struct vm_area_struct, vm_rb);
        // Another means the tree is empty, it's the first vma to insert into the rb tree.
		else
			vma->vm_next = NULL;
	}
}
```

##### 2.2.6 Function: __vma_link_rb() (mm/mmap.c)

It insert the new vma node into the rb_tree, and do the rotate in the `rb_insert_color` function, that's the ability of the rb_tree library, we don't care about the implementation principle of it.

```C
static inline void __vma_link_rb(struct mm_struct * mm, struct vm_area_struct * vma,
				 rb_node_t ** rb_link, rb_node_t * rb_parent)
{
	rb_link_node(&vma->vm_rb, rb_parent, rb_link);
	rb_insert_color(&vma->vm_rb, &mm->mm_rb);
}
```

##### 2.2.7 Function: __vma_link_file() (mm/mmap.c)

This function links the VMA into a linked list of shared file mappings.

```C
static inline void __vma_link_file(struct vm_area_struct * vma)
{
	struct file * file;

	file = vma->vm_file;
    // Check if VMA has a shared file mapping. If not, this function return.
	if (file) {
		struct inode * inode = file->f_dentry->d_inode;
		struct address_space *mapping = inode->i_mapping;
		struct vm_area_struct **head;

		if (vma->vm_flags & VM_DENYWRITE)
			atomic_dec(&inode->i_writecount);

		head = &mapping->i_mmap;
		if (vma->vm_flags & VM_SHARED)
			head = &mapping->i_mmap_shared;
      
		/* insert vma into inode's share list */
		if((vma->vm_next_share = *head) != NULL)
			(*head)->vm_pprev_share = &vma->vm_next_share;
		*head = vma;
		vma->vm_pprev_share = head;
	}
}
```

#### 2.3 Merging Contiguous Regions

##### 2.3.1 Function: vma_merge() (mm/mmap.c)

This function checks to see if a region pointed to be prev may be expanded forward to cover the area from addr to end instead of allocating a new VMA. If it cannot, the VMA ahead is checked to see whether it can be expanded backward instead.

```C
/**
 * @mm: The mm the VMAs belong to.
 * @prev: The VMA before the address we are interested in.
 * @rb_parent: The parent RB node as returned by find_vma_prepare()
 * @addr: The starting address of the region to be merged
 * @end: The end of the region to be merged.
 * @vm_flags: The permission flags of the region to be merged
 * Return: 1 means merge success, 0 means merge failed.
 */
static int vma_merge(struct mm_struct * mm, struct vm_area_struct * prev,
		     rb_node_t * rb_parent, unsigned long addr, unsigned long end, 					unsigned long vm_flags)
{
    // The lock to the mm
	spinlock_t * lock = &mm->page_table_lock;
	if (!prev) {
		prev = rb_entry(rb_parent, struct vm_area_struct, vm_rb);
		goto merge_next;
	}
	if (prev->vm_end == addr && can_vma_merge(prev, vm_flags)) {
		struct vm_area_struct * next;

		spin_lock(lock);
		prev->vm_end = end;
		next = prev->vm_next;
        // The new vma inserted in the middle of the prev and the prev->next, so it can at most merge the two vma.
		if (next && prev->vm_end == next->vm_start && can_vma_merge(next, vm_flags)) {
			prev->vm_end = next->vm_end;
			__vma_unlink(mm, next, prev);
			spin_unlock(lock);

			mm->map_count--;
			kmem_cache_free(vm_area_cachep, next);
			return 1;
		}
		spin_unlock(lock);
		return 1;
	}

    // In previous code, the prev can't merge the current vma. So now try to merge the next vma.
	prev = prev->vm_next;
	if (prev) {
 merge_next:
		if (!can_vma_merge(prev, vm_flags))
			return 0;
		if (end == prev->vm_start) {
			spin_lock(lock);
			prev->vm_start = addr;
			spin_unlock(lock);
			return 1;
		}
	}

	return 0;
}
```

##### 2.3.2 Function: can_vma_merge() (include/linux/mm.h)

Check the flags is equal. Only the same flags vma can be merged.

```C
static inline int can_vma_merge(struct vm_area_struct * vma, unsigned long vm_flags)
{
    // Only anonymous memory regions can be merged.
	if (!vma->vm_file && vma->vm_flags == vm_flags)
		return 1;
	else
		return 0;
}
```

#### 2.4. Remapping and Moving a Memory Region

##### 2.4.1 Function: sys_mremap() (mm/mremap.c)

This is the system service call to remap a memory region. I have not use the system call, so I will learn it later.

#### 2.5 Deleting a Memory Region

##### 2.5.1 Function: do_munmap() (mm/mmap.c)

This function is responsible for unmapping a region. If necessary, the unmapping can span(跨越) multiple VMAs, and it partially unmap one if necessary. So the unmapping operation has been divided into two part. First is finding what VMAs are affected, and `unmap_fixup()` is responsible for fixing up the remaining VMAs.

There some small block and do there own job, we will summary them first:

*  Funtion as a preamble(前言), and find the VMA to start working from.
* Take all VMAs affected by unmapping out of the mm and place them on a linked list headed by the variable free.
* Cycle through the list headed by free, unmap all the pages in the region to be unmapped and call `unmap_fixup` to fix up the mappings.
* Validate the mm and free memory associated with the unmapping.

```C
/* Munmap is split into 2 main parts -- this part which finds
 * what needs doing, and the areas themselves, which do the
 * work.  This now handles partial unmappings.
 * Jeremy Fitzhardine <jeremy@sw.oz.au>
 */
int do_munmap(struct mm_struct *mm, unsigned long addr, size_t len)
{
	struct vm_area_struct *mpnt, *prev, **npp, *free, *extra;

	if ((addr & ~PAGE_MASK) || addr > TASK_SIZE || len > TASK_SIZE-addr)
		return -EINVAL;

	if ((len = PAGE_ALIGN(len)) == 0)
		return -EINVAL;

	/* Check if this memory area is ok - put it on the temporary
	 * list if so..  The checks here are pretty simple --
	 * every area affected in some way (by any overlap) is put
	 * on the list.  If nothing is put on, nothing is affected.
	 */
    // must has addr < mpnt->vm_end, addr may be in the mpnt, or mpnt is behind the mpnt.
	mpnt = find_vma_prev(mm, addr, &prev);
	if (!mpnt)
		return 0;
	/* we have  addr < mpnt->vm_end  */

	if (mpnt->vm_start >= addr+len)
		return 0;

	/* If we'll make "hole", check the vm areas limit */
    // [addr, addr+len)is part of the current mpnt
	if ((mpnt->vm_start < addr && mpnt->vm_end > addr+len)
	    && mm->map_count >= max_map_count)
		return -ENOMEM;

	/*
	 * We may need one additional vma to fix up the mappings ... 
	 * and this is the last chance for an easy error exit.
	 */
	extra = kmem_cache_alloc(vm_area_cachep, SLAB_KERNEL);
	if (!extra)
		return -ENOMEM;

	// npp is the reference of the prev->vm_next.
	npp = (prev ? &prev->vm_next : &mm->mmap);
	free = NULL;
	spin_lock(&mm->page_table_lock);
    // |		|		addr+len		|
   	// addr		mpnt->start		mpnt->end
    // The first mpnt which mpnt->vm_end > addr.
	for ( ; mpnt && mpnt->vm_start < addr+len; mpnt = *npp) {
        // The mpnt->prev->vm_next = mpnt->vm_next. Equals to delete the mpnt from the mmap list.
		*npp = mpnt->vm_next;

        // Add it to the free list
		mpnt->vm_next = free;
		free = mpnt;

        // Remove it from the rb_tree
		rb_erase(&mpnt->vm_rb, &mm->mm_rb);
	}
	mm->mmap_cache = NULL;	/* Kill the cache. */
	spin_unlock(&mm->page_table_lock);

	/* Ok - we have the memory areas we should free on the 'free' list,
	 * so release them, and unmap the page range..
	 * If the one of the segments is only being partially unmapped,
	 * it will put new vm_area_struct(s) into the address space.
	 * In that case we have to be careful with VM_DENYWRITE.
	 */
	while ((mpnt = free) != NULL) {
		unsigned long st, end, size;
		struct file *file = NULL;

		free = free->vm_next;

		st = addr < mpnt->vm_start ? mpnt->vm_start : addr;
		end = addr+len;
		end = end > mpnt->vm_end ? mpnt->vm_end : end;
		size = end - st;

		if (mpnt->vm_flags & VM_DENYWRITE &&
		    (st != mpnt->vm_start || end != mpnt->vm_end) &&
		    (file = mpnt->vm_file) != NULL) {
            // When this field is negative, it counts how many users there are protecting this file from being opened for writing.
			atomic_dec(&file->f_dentry->d_inode->i_writecount);
		}
        // Removes the file mapping.
		remove_shared_vm_struct(mpnt);
		mm->map_count--;

        // Remove the mapping in the page table within this region.
		zap_page_range(mm, st, size);

		/*
		 * Fix the mapping, and free the old area if it wasn't reused.
		 */
		extra = unmap_fixup(mm, mpnt, st, size, extra);
		if (file)
			atomic_inc(&file->f_dentry->d_inode->i_writecount);
	}
	validate_mm(mm);

	/* Release the extra vma struct if it wasn't used */
	if (extra)
		kmem_cache_free(vm_area_cachep, extra);

    // Free all the pagetables that were used for the unmapped region
	free_pgtables(mm, prev, addr, addr+len);

	return 0;
}
```

##### 2.5.2 Function: unmap_fixup() (mm/mmap.c)

This function fixes up the regions after a block has been unmapped. It is passed a list of VMAs that are affected by the unmapping, the region and length to be unmapped and a spare VMA that may be required to fix up the region if a whole is created. This function handles four principle cases:

1. The unmapping of a region.
2. Partial unmapping from the start to somewhere in the middle.
3. Partial unmapping from somewhere in the middle to the end.
4. Creation of a hole in the middle of the region.

```C
static struct vm_area_struct * unmap_fixup(struct mm_struct *mm, 
	struct vm_area_struct *area, unsigned long addr, size_t len, 
	struct vm_area_struct *extra)
{
	struct vm_area_struct *mpnt;
	unsigned long end = addr + len;

	area->vm_mm->total_vm -= len >> PAGE_SHIFT;
	if (area->vm_flags & VM_LOCKED)
		area->vm_mm->locked_vm -= len >> PAGE_SHIFT;

	/* Unmapping the whole area. */
	if (addr == area->vm_start && end == area->vm_end) {
		if (area->vm_ops && area->vm_ops->close)
			area->vm_ops->close(area);
		if (area->vm_file)
			fput(area->vm_file);
		kmem_cache_free(vm_area_cachep, area);
		return extra;
	}

	/* Work out to one of the ends. */
	if (end == area->vm_end) {
		/*
		 * here area isn't visible to the semaphore-less readers
		 * so we don't need to update it under the spinlock.
		 */
		area->vm_end = addr;
		lock_vma_mappings(area);
		spin_lock(&mm->page_table_lock);
	} else if (addr == area->vm_start) {
		area->vm_pgoff += (end - area->vm_start) >> PAGE_SHIFT;
		/* same locking considerations of the above case */
		area->vm_start = end;
		lock_vma_mappings(area);
		spin_lock(&mm->page_table_lock);
	} else {
	/* Unmapping a hole: area->vm_start < addr <= end < area->vm_end */
		/* Add end mapping -- leave beginning for below */
		mpnt = extra;
		extra = NULL;

		mpnt->vm_mm = area->vm_mm;
		mpnt->vm_start = end;
		mpnt->vm_end = area->vm_end;
		mpnt->vm_page_prot = area->vm_page_prot;
		mpnt->vm_flags = area->vm_flags;
		mpnt->vm_raend = 0;
		mpnt->vm_ops = area->vm_ops;
		mpnt->vm_pgoff = area->vm_pgoff + ((end - area->vm_start) >> PAGE_SHIFT);
		mpnt->vm_file = area->vm_file;
		mpnt->vm_private_data = area->vm_private_data;
		if (mpnt->vm_file)
			get_file(mpnt->vm_file);
		if (mpnt->vm_ops && mpnt->vm_ops->open)
			mpnt->vm_ops->open(mpnt);
		area->vm_end = addr;	/* Truncate area */

		/* Because mpnt->vm_file == area->vm_file this locks
		 * things correctly.
		 */
		lock_vma_mappings(area);
		spin_lock(&mm->page_table_lock);
		__insert_vm_struct(mm, mpnt);
	}

	__insert_vm_struct(mm, area);
	spin_unlock(&mm->page_table_lock);
	unlock_vma_mappings(area);
	return extra;
}
```

### 4. Locking and Unlocking Memory regions

#### 4.1 Locking a Memory Region

##### 4.1.1 Function: sys_mlock() (mm/mlock.c)

...

### 5. Page Faulting

There are two types of page fault, major and minor faults.

* Major page faults: Expensive operation. Occur when data has to be read from disk.
* Minor page faults: Other faults, we also call it soft page fault.

| Exception                                                    | Type  | Action                                                       |
| ------------------------------------------------------------ | ----- | ------------------------------------------------------------ |
| Region valid but page not allocated                          | Minor | Allocate a page frame from the physical page allocator       |
| Region not valid but is beside an expandable region like the stack | Minor | Expand the region and allocate a page                        |
| Page swapped out but present in swap cache                   | Minor | Re-establish the page in the process page tables and drop a reference to the swap cache |
| Page swapped out to backing storage                          | Major | Find where the page with information stored in the PTE and read it from disk |
| Page write when marked read-only                             | Minor | If the page is a COW page, make a copy of it, mark it writable and assign it to the process. If it is in fact a bad write, send a SIGSEGV signal |
| Region is invalid or process has no permissions to access    | Error | Send a SEGSEGV signal to the process                         |
| Fault occurred in the kernel portion address space           | Minor | If the fault occurred in the vmalloc area of the address space, the current process page tables are updated against the master page table held by init_mm. This is the only valid kernel page fault that may occur. |
| Fault occurred in the userspace region whil in kernel mode   | Error | If a fault occurs, it means a kernel system did not copy from userspace properly and caused a page fault. This is a kernel bug which is treated quite severely(严重). |

#### 5.1 x86 Page Fault Handler

##### 5.1.1 Function: do_page_fault (arch/i386/mm/fault.c)

```C
140 asmlinkage void do_page_fault(struct pt_regs *regs, 
                  unsigned long error_code)
141 {
142     struct task_struct *tsk;
143     struct mm_struct *mm;
144     struct vm_area_struct * vma;
145     unsigned long address;
146     unsigned long page;
147     unsigned long fixup;
148     int write;
149     siginfo_t info;
150 
151     /* get the address */
152     __asm__("movl %%cr2,%0":"=r" (address));
153 
154     /* It's safe to allow irq's after cr2 has been saved */
155     if (regs->eflags & X86_EFLAGS_IF)
156         local_irq_enable();
157 
158     tsk = current;
159 
    // Means the vmalloc fault
173     if (address >= TASK_SIZE && !(error_code & 5))
174         goto vmalloc_fault;
175 
176     mm = tsk->mm;
177     info.si_code = SEGV_MAPERR;
178 
    // If this is in interrupt, or there is no memory context (such as with a kernel thread), there is no way to safely handle the fault.
183     if (in_interrupt() || !mm)
184         goto no_context;
185 
    // At here, we can know the fault is in userspace.
    // Find the VMA for the faulting address and determine if it is a good area, a bad area or if the fault occurred near a region that can be expanded such as the stack.
186     down_read(&mm->mmap_sem);
187 
    // Find the VMA that is responsible or is closest to the faulting address.
188     vma = find_vma(mm, address);
189     if (!vma)
190         goto bad_area;
    // Find the VMA, and addr in it, it's good_area.
191     if (vma->vm_start <= address)
192         goto good_area;
    // For the region that is closest, check if it can grown down (VM_GROWDOWN). If it does, it means the stack can probably be expanded.
193     if (!(vma->vm_flags & VM_GROWSDOWN))
194         goto bad_area;
    // If the error_code is 4, it means it is running in userspace, and make sure it isn't an access below the stack.
195     if (error_code & 4) {
196         /*
197          * accessing the stack below %esp is always a bug.
198          * The "+ 32" is there due to some instructions (like
199          * pusha) doing post-decrement on the stack and that
200          * doesn't show up until later..
201          */
202         if (address + 32 < regs->esp)
203             goto bad_area;
204     }
205     if (expand_stack(vma, address))
206         goto bad_area;
    
211 good_area:
    // By default return an error
212     info.si_code = SEGV_ACCERR;
213     write = 0;
    /*
     * The error_code.
     * Bit(0): 0 means page was not present. 1 means a protection fault.
     * Bit(1): 0 means read fault, 1 means write fault.
     */
214     switch (error_code & 3) {
        // Write protection fault
215         default:    /* 3: write, present */
216 #ifdef TEST_VERIFY_AREA
217             if (regs->cs == KERNEL_CS)
218                 printk("WP fault at %08lx\n", regs->eip);
219 #endif
220             /* fall through */
221         case 2:     /* write, not present */
        // If the region can not be written to, it is a bad write.
        // If the region can be written to, this is a page that is marked Copy On Write(COW)
222             if (!(vma->vm_flags & VM_WRITE))
223                 goto bad_area;
224             write++;
225             break;
226         case 1:     /* read, present */
        // Should never gonna happen.
227             goto bad_area;
228         case 0:     /* read, not present */
        // A read occured on a missing page. Make sure it is ok to read or exec this page. The check for exec is made because the x86 can not exec protect a page and instead uses the read protect flag. This is why both have to be checked.
229             if (!(vma->vm_flags & (VM_READ | VM_EXEC)))
230                 goto bad_area;
231     }
    
233  survive:
239     switch (handle_mm_fault(mm, vma, address, write)) {
        // Return 1 means it was a minor fault.
240     case 1:
241         tsk->min_flt++;
242         break;
        // Return 2 means it was a major fault.
243     case 2:
244         tsk->maj_flt++;
245         break;
        // Return 0 means some IO error happeded during the fault.
246     case 0:
247         goto do_sigbus;
        // Other means memory could not be allocated for the fault. In reality this does not happen as another function out_of_memory() is invoked in mm/oom_kill.c before this could happen which is a lot more graceful about who it kills.
248     default:
249         goto out_of_memory;
250     }
251 
252     /*
253      * Did it hit the DOS screen memory VA from vm86 mode?
254      */
255     if (regs->eflags & VM_MASK) {
256         unsigned long bit = (address - 0xA0000) >> PAGE_SHIFT;
257         if (bit < 32)
258             tsk->thread.screen_bitmap |= 1 << bit;
259     }
260     up_read(&mm->mmap_sem);
    // Return as the fault has been successfully handled.
261     return;

267 bad_area:
268     up_read(&mm->mmap_sem);
269 
270     /* User mode accesses just cause a SIGSEGV */
271     if (error_code & 4) {
272         tsk->thread.cr2 = address;
273         tsk->thread.error_code = error_code;
274         tsk->thread.trap_no = 14;
275         info.si_signo = SIGSEGV;
276         info.si_errno = 0;
277         /* info.si_code has been set above */
278         info.si_addr = (void *)address;
    // Send the SIGSEGV signal. The process will exit and dump all the relevant information.
279         force_sig_info(SIGSEGV, &info, tsk);
280         return;
281     }
282 

296 
    // This no_context handler must handle a kernel page fault.
297 no_context:
    // This is very important during copy_from_user() and copy_to_user() and when they reads and writes to invalid regions, there has a change to call a small fixup code rather than falling through to the next block which causes an oops.
298     /* Are we prepared to handle this kernel fault?  */
299     if ((fixup = search_exception_table(regs->eip)) != 0) {
300         regs->eip = fixup;
301         return;
302     }
    
304 /*
305  * Oops. The kernel tried to access some bad page. We'll have to
306  * terminate things with extreme prejudice.
307  */
308 
309     bust_spinlocks(1);
310 
311     if (address < PAGE_SIZE)
312         printk(KERN_ALERT "Unable to handle kernel NULL pointer
                     dereference");
313     else
314         printk(KERN_ALERT "Unable to handle kernel paging
                     request");
315     printk(" at virtual address %08lx\n",address);
316     printk(" printing eip:\n");
317     printk("%08lx\n", regs->eip);
318     asm("movl %%cr3,%0":"=r" (page));
319     page = ((unsigned long *) __va(page))[address >> 22];
320     printk(KERN_ALERT "*pde = %08lx\n", page);
321     if (page & 1) {
322         page &= PAGE_MASK;
323         address &= 0x003ff000;
324         page = ((unsigned long *) 
                __va(page))[address >> PAGE_SHIFT];
325         printk(KERN_ALERT "*pte = %08lx\n", page);
326     }
327     die("Oops", regs, error_code);
328     bust_spinlocks(0);
329     do_exit(SIGKILL);
                   
335 out_of_memory:
336     if (tsk->pid == 1) {
337         yield();
338         goto survive;
339     }
340     up_read(&mm->mmap_sem);
341     printk("VM: killing process %s\n", tsk->comm);
342     if (error_code & 4)
343         do_exit(SIGKILL);
344     goto no_context;
                   
345 
346 do_sigbus:
347     up_read(&mm->mmap_sem);
348 
353     tsk->thread.cr2 = address;
354     tsk->thread.error_code = error_code;
355     tsk->thread.trap_no = 14;
356     info.si_signo = SIGBUS;
357     info.si_errno = 0;
358     info.si_code = BUS_ADRERR;
359     info.si_addr = (void *)address;
360     force_sig_info(SIGBUS, &info, tsk);
361 
362     /* Kernel mode? Handle exceptions or die */
363     if (!(error_code & 4))
364         goto no_context;
365     return;

367 vmalloc_fault:
368     {
376         int offset = __pgd_offset(address);
377         pgd_t *pgd, *pgd_k;
378         pmd_t *pmd, *pmd_k;
379         pte_t *pte_k;
380 
381         asm("movl %%cr3,%0":"=r" (pgd));
382         pgd = offset + (pgd_t *)__va(pgd);
383         pgd_k = init_mm.pgd + offset;
384 
385         if (!pgd_present(*pgd_k))
386             goto no_context;
387         set_pgd(pgd, *pgd_k);
388         
389         pmd = pmd_offset(pgd, address);
390         pmd_k = pmd_offset(pgd_k, address);
391         if (!pmd_present(*pmd_k))
392             goto no_context;
393         set_pmd(pmd, *pmd_k);
394 
395         pte_k = pte_offset(pmd_k, address);
396         if (!pte_present(*pte_k))
397             goto no_context;
398         return;
399     }
400 }
```

#### 5.2 Expanding the Stack

##### 5.2.1 Function: expand_stack (include/linux/mm.h)

The VMA supplied is guaranteed to be one that can grow to cover the address.

```C
640 static inline int expand_stack(struct vm_area_struct * vma, 
                                   unsigned long address)
641 {
642     unsigned long grow;
643 
644     /*
645      * vma->vm_start/vm_end cannot change under us because 
         * the caller is required
646      * to hold the mmap_sem in write mode. We need to get the
647      * spinlock only before relocating the vma range ourself.
648      */
649     address &= PAGE_MASK;
650     spin_lock(&vma->vm_mm->page_table_lock);
651     grow = (vma->vm_start - address) >> PAGE_SHIFT;
652     if (vma->vm_end - address > current->rlim[RLIMIT_STACK].rlim_cur ||
653     ((vma->vm_mm->total_vm + grow) << PAGE_SHIFT) > 
                                       current->rlim[RLIMIT_AS].rlim_cur) {
654         spin_unlock(&vma->vm_mm->page_table_lock);
655         return -ENOMEM;
656     }
657     vma->vm_start = address;
658     vma->vm_pgoff -= grow;
659     vma->vm_mm->total_vm += grow;
660     if (vma->vm_flags & VM_LOCKED)
661         vma->vm_mm->locked_vm += grow;
662     spin_unlock(&vma->vm_mm->page_table_lock);
663     return 0;
664 }
```

