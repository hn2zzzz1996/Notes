# 内核的执行入口

接上回，内核首先被加载到`0x1000000`这个物理地址的地方，并且现在的虚拟地址与物理地址的映射是一一对应的，所以这个时候的虚拟地址还是`0x1000000`，对应着**arch/x86/kernel/head_64.S**中的**startup_64**函数。

因为在链接的时候指定的内核虚拟地址是`0xffffffff81000000`，所以调试的时候如果直接把断点打在`startup_64`这个函数上，那么实际断点的地址其实是`0xffffffff81000000`，而此时内核的虚拟地址是`0x1000000`，所以会断不下来。所以我们直接在地址上打下断点：

```Shell
hb *0x1000000
```

## 内核地址的宏定义

```C
#define __START_KERNEL_map	_AC(0xffffffff80000000, UL)

// CONFIG_PHYSICAL_START is 0x1000000
#define __PHYSICAL_START  ALIGN(CONFIG_PHYSICAL_START, CONFIG_PHYSICAL_ALIGN)

// 0xffffffff81000000
#define __START_KERNEL    (__START_KERNEL_map + __PHYSICAL_START)
```

上面的定义表示着：

* **__START_KERNEL_map**: 内核虚拟地址与物理地址的偏移 - `0xffffffff80000000`

* **__PHYSICAL_START**: 内核要加载到的物理地址 - `0x1000000`
* **__START_KERNEL**: 内核要加载到的虚拟地址 - `0xffffffff81000000`

## kernel的入口

下面我们来看一下内核的入口，也就是`__PHYSICAL_START`这个地方最开始执行的指令：

```C
// arch/x86/kernel/head_64.S
	__HEAD
	.code64
SYM_CODE_START_NOALIGN(startup_64)
    /*
	 * At this point the CPU runs in 64bit mode CS.L = 1 CS.D = 0,
	 * and someone has loaded an identity mapped page table
	 * for us.  These identity mapped page tables map all of the
	 * kernel pages and possibly all of memory.
	 *
	 * %rsi holds a physical pointer to real_mode_data.
	 */
        
    leaq	_text(%rip), %rdi
	pushq	%rsi
	call	startup_64_setup_env
	popq	%rsi
```

首先开头的`__HEAD`意味着这段代码会被编译到`.head.text`段中，在链接的时候也就位于整个内核代码的最前面，也才决定了当从boot代码跳转过来的时候刚开始执行的就是这里的代码。

首先第一条指令`leaq	_text(%rip), %rdi`得到了当前被加载到的物理地址是多少，因为：

```C
.text :  AT(ADDR(.text) - LOAD_OFFSET) {
    _text = .;
    HEAD_TEXT
    ...
}
```

这里`_text`代表了内核在内存中开始的地址是多少，因为这里采用了相对寻址的方式，所以实际得到的是`_text`的物理地址，也就是内核被加载到的物理地址。

**rsi**: 指向了boot_params.

然后调用`startup_64_setup_env`去加载新的gdt：

```C
/*
 * Setup boot CPU state needed before kernel switches to virtual addresses.
 */
void __head startup_64_setup_env(unsigned long physbase)
{
	/* Load GDT */
	startup_gdt_descr.address = (unsigned long)fixup_pointer(startup_gdt, physbase);
	native_load_gdt(&startup_gdt_descr);

	/* New GDT is live - reload data segment registers */
	asm volatile("movl %%eax, %%ds\n"
		     "movl %%eax, %%ss\n"
		     "movl %%eax, %%es\n" : : "a"(__KERNEL_DS) : "memory");

	startup_64_load_idt(physbase);
}
```

这里唯一要注意的就是`fixup_pointer`函数，这个函数是为了修正地址的，因为此时的地址与编译期间的地址不同，所以要进行地址的修正：

```C
static void __head *fixup_pointer(void *ptr, unsigned long physaddr)
{
	return ptr - (void *)_text + (void *)physaddr;
}
```

可以看到，这里的修正也很简单，就是直接用内核被加载的物理地址加上变量的地址相对于`_text`的偏移。

然后将修正过后的`startup_gdt`的地址填入到`startup_gdt_descr.address`中，然后加载新的gdt。

然后切换到新的段寄存器：

```C
/* Now switch to __KERNEL_CS so IRET works reliably */
	pushq	$__KERNEL_CS
	leaq	.Lon_kernel_cs(%rip), %rax
	pushq	%rax
	lretq

.Lon_kernel_cs:
```

接下来继续看，调用了`__startup_64`函数：

```C
.Lon_kernel_cs:
	leaq	_text(%rip), %rdi
	pushq	%rsi
	call	__startup_64
	popq	%rsi
```

`__startup_64`的两个参数分别对应着两个寄存器：

* @physaddr: 对应寄存器`rdi`，保存了内核被加载的物理地址
* @bp: 对应寄存器`rsi`, 内核参数的地址

```C
// arch/x86/kernel/head64.c
unsigned long __head __startup_64(unsigned long physaddr,
				  struct boot_params *bp)
{
    unsigned long vaddr, vaddr_end;
	unsigned long load_delta, *p;
	unsigned long pgtable_flags;
	pgdval_t *pgd;
	p4dval_t *p4d;
	pudval_t *pud;
	pmdval_t *pmd, pmd_entry;
	pteval_t *mask_ptr;
	bool la57;
	int i;
	unsigned int *next_pgt_ptr;
    ... 
    load_delta = physaddr - (unsigned long)(_text - __START_KERNEL_map);
}
```

首先是计算了一个`load_delta`，如果开启了`ASLR`地址随机化的话，那么实际加载的物理地址和编译期间的物理地址是不一样的，所以计算一个偏移。`_text - __START_KERNEL_map`其实就是`__PHYSICAL_START`，等于`0x1000000`，如果我们没开启`ASLR`的话，`load_delta`就是0。

然后会去建立一个新的页表，给内核地址空间`__START_KERNEL_map`起始的虚拟地址空间建立相应的映射。

```C
unsigned long __head __startup_64(unsigned long physaddr,
				  struct boot_params *bp)
{
    ...
    pgd = fixup_pointer(&early_top_pgt, physaddr);
	p = pgd + pgd_index(__START_KERNEL_map);
    
    *p = (unsigned long)level3_kernel_pgt;
    *p += _PAGE_TABLE_NOENC - __START_KERNEL_map + load_delta;
}
```

这里新的页表的cr3指向的是`early_top_pgt`，然后通过`pgd_index`计算出内核虚拟地址偏移的大小，这个地方计算出来应该是511，也就是最后一项。然后让pgd[511]指向`level3_kernel_pgt`，因为实际页表中要保存的是物理地址，但是引用`level3_kernel_pgt`是虚拟地址，所以计算出实际的物理地址。

```C
SYM_DATA_START_PAGE_ALIGNED(level3_kernel_pgt)
	.fill	L3_START_KERNEL,8,0
	/* (2^48-(2*1024*1024*1024)-((2^39)*511))/(2^30) = 510 */
	.quad	level2_kernel_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
	.quad	level2_fixmap_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
SYM_DATA_END(level3_kernel_pgt)
```

然后`level2_kernel_pgt`直接通过一个循环建立最终的映射，如果开启了KASLR，内核虚拟地址空间大小为1GB，否则为512MB：

```C
SYM_DATA_START_PAGE_ALIGNED(level2_kernel_pgt)
	/*
	 * Kernel high mapping.
	 *
	 * The kernel code+data+bss must be located below KERNEL_IMAGE_SIZE in
	 * virtual address space, which is 1 GiB if RANDOMIZE_BASE is enabled,
	 * 512 MiB otherwise.
	 *
	 * (NOTE: after that starts the module area, see MODULES_VADDR.)
	 *
	 * This table is eventually used by the kernel during normal runtime.
	 * Care must be taken to clear out undesired bits later, like _PAGE_RW
	 * or _PAGE_GLOBAL in some cases.
	 */
	PMDS(0, __PAGE_KERNEL_LARGE_EXEC, KERNEL_IMAGE_SIZE/PMD_SIZE)
SYM_DATA_END(level2_kernel_pgt)
```

然后再为了切换到虚拟地址执行的代码能够顺利执行，建立`identity`的映射，并且多建立一点儿冗余的entry，保证映射是正确的。

构建出来的页表如下图所示：

![early_top_pgt](.\picture\early_top_pgt.png)

最后再把`phys_base`的值修正，也就是内核加载到的物理地址是多少：

```C
	/*
	 * Fixup phys_base - remove the memory encryption mask to obtain
	 * the true physical address.
	 */
	*fixup_long(&phys_base, physaddr) += load_delta - sme_get_me_mask();
```

接着返回到`head_64.S`中继续执行：

```C
/* Form the CR3 value being sure to include the CR3 modifier */
	addq	$(early_top_pgt - __START_KERNEL_map), %rax
	jmp 1f
```

这段代码是把`early_top_pgt`的偏移保存到了`rax`寄存器中，然后跳转到下面的代码继续执行：

```C
1:

	/* Enable PAE mode, PGE and LA57 */
	movl	$(X86_CR4_PAE | X86_CR4_PGE), %ecx
	movq	%rcx, %cr4

	/* Setup early boot stage 4-/5-level pagetables. */
	addq	phys_base(%rip), %rax
        
    /* Switch to new page-table */
	movq	%rax, %cr3
```

可以看到，这里打开了分页，并且把`phys_base`的值也就是内核加载到的物理地址的值加到了`rax`上，那么此时`rax`中保存的就是`early_top_pgt`的物理地址，然后切换到新的页表。

切换到新的页表之后，还需要切换到新的虚拟地址去执行，看如下代码，首先是将`1f`的虚拟地址给了rax，然后跳转到内核的虚拟地址去执行。

```C
/* Ensure I am executing from virtual addresses */
	movq	$1f, %rax
	ANNOTATE_RETPOLINE_SAFE
	jmp	*%rax
1:
	lgdt	early_gdt_descr(%rip)

	/* set up data segments */
	xorl %eax,%eax
	movl %eax,%ds
	movl %eax,%ss
	movl %eax,%es
```

可以看到，在`1`标号之后的代码，都已经运行在了内核配置的虚拟地址空间中，这里要加载新的gdt，因为之前加载的gdt的地址在之后已经是userspace的地址空间了，内核不应该访问这段地址空间，所以要及时的切换到自己的gdt。

这里`early_gdt_descr`指向的是`gdt_page`：

```C
SYM_DATA(early_gdt_descr,		.word GDT_ENTRIES*8-1)
SYM_DATA_LOCAL(early_gdt_descr_base,	.quad INIT_PER_CPU_VAR(gdt_page))
```

而`gdt_page`是一个percpu的变量，其在定义的时候被初始化：

```C
// arch/x86/kernel/cpu/common.c
DEFINE_PER_CPU_PAGE_ALIGNED(struct gdt_page, gdt_page) = { .gdt = {
	/*
	 * We need valid kernel segments for data and code in long mode too
	 * IRET will check the segment types  kkeil 2000/10/28
	 * Also sysret mandates a special GDT layout
	 *
	 * TLS descriptors are currently at a different place compared to i386.
	 * Hopefully nobody expects them at a fixed place (Wine?)
	 */
	[GDT_ENTRY_KERNEL32_CS]		= GDT_ENTRY_INIT(0xc09b, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_CS]		= GDT_ENTRY_INIT(0xa09b, 0, 0xfffff),
	[GDT_ENTRY_KERNEL_DS]		= GDT_ENTRY_INIT(0xc093, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER32_CS]	= GDT_ENTRY_INIT(0xc0fb, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_DS]	= GDT_ENTRY_INIT(0xc0f3, 0, 0xfffff),
	[GDT_ENTRY_DEFAULT_USER_CS]	= GDT_ENTRY_INIT(0xa0fb, 0, 0xfffff),
} };
```

设置了gdt之后，对percpu的gs寄存器也进行了设置：

```C
/* Set up %gs.
	 *
	 * The base of %gs always points to fixed_percpu_data. If the
	 * stack protector canary is enabled, it is located at %gs:40.
	 * Note that, on SMP, the boot cpu uses init data section until
	 * the per cpu areas are set up.
	 */
	movl	$MSR_GS_BASE,%ecx
	movl	initial_gs(%rip),%eax
	movl	initial_gs+4(%rip),%edx
	wrmsr
```

然后切换到C代码去运行：

```C
	pushq	$.Lafter_lret	# put return address on stack for unwinder
	xorl	%ebp, %ebp	# clear frame pointer
	movq	initial_code(%rip), %rax
	pushq	$__KERNEL_CS	# set correct cs
	pushq	%rax		# target address in negative space
	lretq
```

这里`initial_code`就是**x86_64_start_kernel**。

```
SYM_DATA(initial_code,	.quad x86_64_start_kernel)
SYM_DATA(initial_gs,	.quad INIT_PER_CPU_VAR(fixed_percpu_data))
```

